{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationCode</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>WindSpeed(m/s)</th>\n",
       "      <th>Pressure(hpa)</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Sunlight(Lux)</th>\n",
       "      <th>Power(mW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:14:06.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>15.59</td>\n",
       "      <td>94.30</td>\n",
       "      <td>652.92</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:14:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>15.66</td>\n",
       "      <td>94.04</td>\n",
       "      <td>682.50</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:15:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.47</td>\n",
       "      <td>15.74</td>\n",
       "      <td>94.10</td>\n",
       "      <td>750.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:16:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.46</td>\n",
       "      <td>15.78</td>\n",
       "      <td>94.09</td>\n",
       "      <td>738.33</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:17:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.49</td>\n",
       "      <td>15.80</td>\n",
       "      <td>94.08</td>\n",
       "      <td>660.83</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89607</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:50:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.54</td>\n",
       "      <td>30.69</td>\n",
       "      <td>72.91</td>\n",
       "      <td>2288.33</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89608</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:51:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.40</td>\n",
       "      <td>30.27</td>\n",
       "      <td>73.16</td>\n",
       "      <td>3236.67</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89609</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:52:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.39</td>\n",
       "      <td>29.90</td>\n",
       "      <td>72.51</td>\n",
       "      <td>4526.67</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89610</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:53:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.40</td>\n",
       "      <td>29.38</td>\n",
       "      <td>73.23</td>\n",
       "      <td>4231.67</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89611</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:54:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.58</td>\n",
       "      <td>29.06</td>\n",
       "      <td>74.22</td>\n",
       "      <td>3685.83</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290894 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LocationCode                 DateTime  WindSpeed(m/s)  Pressure(hpa)  \\\n",
       "0                10  2024-03-01 17:14:06.000             0.0        1017.48   \n",
       "1                10  2024-03-01 17:14:47.000             0.0        1017.48   \n",
       "2                10  2024-03-01 17:15:47.000             0.0        1017.47   \n",
       "3                10  2024-03-01 17:16:47.000             0.0        1017.46   \n",
       "4                10  2024-03-01 17:17:47.000             0.0        1017.49   \n",
       "...             ...                      ...             ...            ...   \n",
       "89607             9  2024-07-23 15:50:57.000             0.0         994.54   \n",
       "89608             9  2024-07-23 15:51:57.000             0.0         994.40   \n",
       "89609             9  2024-07-23 15:52:57.000             0.0         994.39   \n",
       "89610             9  2024-07-23 15:53:57.000             0.0         994.40   \n",
       "89611             9  2024-07-23 15:54:57.000             0.0         994.58   \n",
       "\n",
       "       Temperature(°C)  Humidity(%)  Sunlight(Lux)  Power(mW)  \n",
       "0                15.59        94.30         652.92       0.12  \n",
       "1                15.66        94.04         682.50       0.12  \n",
       "2                15.74        94.10         750.00       0.14  \n",
       "3                15.78        94.09         738.33       0.14  \n",
       "4                15.80        94.08         660.83       0.12  \n",
       "...                ...          ...            ...        ...  \n",
       "89607            30.69        72.91        2288.33       1.10  \n",
       "89608            30.27        73.16        3236.67       1.92  \n",
       "89609            29.90        72.51        4526.67       3.57  \n",
       "89610            29.38        73.23        4231.67       3.13  \n",
       "89611            29.06        74.22        3685.83       2.46  \n",
       "\n",
       "[1290894 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_list = os.listdir(\"train\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in csv_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_temp = pd.read_csv(f\"train/{file}\")\n",
    "        df = pd.concat([df, df_temp])\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationCode</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>WindSpeed(m/s)</th>\n",
       "      <th>Pressure(hpa)</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Sunlight(Lux)</th>\n",
       "      <th>Power(mW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.754370</td>\n",
       "      <td>2024-03-01 17:14:06.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.487473</td>\n",
       "      <td>-1.661978</td>\n",
       "      <td>1.003364</td>\n",
       "      <td>-0.714856</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.754370</td>\n",
       "      <td>2024-03-01 17:14:47.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.487473</td>\n",
       "      <td>-1.654168</td>\n",
       "      <td>0.992914</td>\n",
       "      <td>-0.713834</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.754370</td>\n",
       "      <td>2024-03-01 17:15:47.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.486877</td>\n",
       "      <td>-1.645242</td>\n",
       "      <td>0.995326</td>\n",
       "      <td>-0.711501</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.754370</td>\n",
       "      <td>2024-03-01 17:16:47.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.486282</td>\n",
       "      <td>-1.640779</td>\n",
       "      <td>0.994924</td>\n",
       "      <td>-0.711904</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.754370</td>\n",
       "      <td>2024-03-01 17:17:47.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>0.488069</td>\n",
       "      <td>-1.638547</td>\n",
       "      <td>0.994522</td>\n",
       "      <td>-0.714583</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89607</th>\n",
       "      <td>-1.254433</td>\n",
       "      <td>2024-07-23 15:50:57.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>-0.879118</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.143706</td>\n",
       "      <td>-0.658330</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89608</th>\n",
       "      <td>-1.254433</td>\n",
       "      <td>2024-07-23 15:51:57.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>-0.887458</td>\n",
       "      <td>-0.024061</td>\n",
       "      <td>0.153753</td>\n",
       "      <td>-0.625551</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89609</th>\n",
       "      <td>-1.254433</td>\n",
       "      <td>2024-07-23 15:52:57.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>-0.888054</td>\n",
       "      <td>-0.065344</td>\n",
       "      <td>0.127630</td>\n",
       "      <td>-0.580963</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89610</th>\n",
       "      <td>-1.254433</td>\n",
       "      <td>2024-07-23 15:53:57.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>-0.887458</td>\n",
       "      <td>-0.123362</td>\n",
       "      <td>0.156566</td>\n",
       "      <td>-0.591160</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89611</th>\n",
       "      <td>-1.254433</td>\n",
       "      <td>2024-07-23 15:54:57.000</td>\n",
       "      <td>-0.420823</td>\n",
       "      <td>-0.876735</td>\n",
       "      <td>-0.159066</td>\n",
       "      <td>0.196354</td>\n",
       "      <td>-0.610026</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290894 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LocationCode                 DateTime  WindSpeed(m/s)  Pressure(hpa)  \\\n",
       "0         -0.754370  2024-03-01 17:14:06.000       -0.420823       0.487473   \n",
       "1         -0.754370  2024-03-01 17:14:47.000       -0.420823       0.487473   \n",
       "2         -0.754370  2024-03-01 17:15:47.000       -0.420823       0.486877   \n",
       "3         -0.754370  2024-03-01 17:16:47.000       -0.420823       0.486282   \n",
       "4         -0.754370  2024-03-01 17:17:47.000       -0.420823       0.488069   \n",
       "...             ...                      ...             ...            ...   \n",
       "89607     -1.254433  2024-07-23 15:50:57.000       -0.420823      -0.879118   \n",
       "89608     -1.254433  2024-07-23 15:51:57.000       -0.420823      -0.887458   \n",
       "89609     -1.254433  2024-07-23 15:52:57.000       -0.420823      -0.888054   \n",
       "89610     -1.254433  2024-07-23 15:53:57.000       -0.420823      -0.887458   \n",
       "89611     -1.254433  2024-07-23 15:54:57.000       -0.420823      -0.876735   \n",
       "\n",
       "       Temperature(°C)  Humidity(%)  Sunlight(Lux)  Power(mW)  \n",
       "0            -1.661978     1.003364      -0.714856       0.12  \n",
       "1            -1.654168     0.992914      -0.713834       0.12  \n",
       "2            -1.645242     0.995326      -0.711501       0.14  \n",
       "3            -1.640779     0.994924      -0.711904       0.14  \n",
       "4            -1.638547     0.994522      -0.714583       0.12  \n",
       "...                ...          ...            ...        ...  \n",
       "89607         0.022800     0.143706      -0.658330       1.10  \n",
       "89608        -0.024061     0.153753      -0.625551       1.92  \n",
       "89609        -0.065344     0.127630      -0.580963       3.57  \n",
       "89610        -0.123362     0.156566      -0.591160       3.13  \n",
       "89611        -0.159066     0.196354      -0.610026       2.46  \n",
       "\n",
       "[1290894 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.LeaveOneOutEncoder(cols=[\"LocationCode\"], sigma = 0.05)\n",
    "encoder.fit(df, df['Power(mW)'])\n",
    "df = encoder.transform(df)\n",
    "\n",
    "# 指定要標準化的欄位\n",
    "columns_to_standardize = ['WindSpeed(m/s)', 'Pressure(hpa)', 'Temperature(°C)', 'Humidity(%)', 'Sunlight(Lux)', 'LocationCode']\n",
    "\n",
    "# 初始化 StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 對指定欄位進行標準化\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_data_with_datetime(df: pd.DataFrame):\n",
    "    op_df = list(pd.to_datetime(df['DateTime']).dt.day)\n",
    "    \n",
    "    data_df = df.drop(columns=['DateTime', 'Power(mW)', 'LocationCode'])\n",
    "    label_df = df['Power(mW)']\n",
    "\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    last_index = 0\n",
    "    for i in range(1, len(op_df) - 1):\n",
    "        if op_df[i] != op_df[i - 1]:\n",
    "            data_list.append(torch.from_numpy(np.array(data_df.iloc[last_index: i])))\n",
    "            label_list.append(torch.from_numpy(np.array(label_df.iloc[last_index:i])))\n",
    "            last_index = i\n",
    "    return data_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([52, 5]), torch.Size([52]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list, label_list = spilt_data_with_datetime(df)\n",
    "\n",
    "data_list[0].shape, label_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24207 (\\N{CJK UNIFIED IDEOGRAPH-5E8F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21015 (\\N{CJK UNIFIED IDEOGRAPH-5217}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 38263 (\\N{CJK UNIFIED IDEOGRAPH-9577}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24230 (\\N{CJK UNIFIED IDEOGRAPH-5EA6}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 22823 (\\N{CJK UNIFIED IDEOGRAPH-5927}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 26044 (\\N{CJK UNIFIED IDEOGRAPH-65BC}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21644 (\\N{CJK UNIFIED IDEOGRAPH-548C}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 23567 (\\N{CJK UNIFIED IDEOGRAPH-5C0F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 31561 (\\N{CJK UNIFIED IDEOGRAPH-7B49}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 20363 (\\N{CJK UNIFIED IDEOGRAPH-4F8B}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 假設這是你的字串陣列\n",
    "\n",
    "# 計算每個字串的長度\n",
    "lengths = [len(s) for s in data_list]\n",
    "\n",
    "# 設定臨界長度\n",
    "threshold = 100\n",
    "\n",
    "# 計算大於和小於等於 threshold 的比例\n",
    "greater_count = sum(1 for length in lengths if length > threshold)\n",
    "less_equal_count = sum(1 for length in lengths if length <= threshold)\n",
    "\n",
    "# 繪製圓餅圖\n",
    "labels = [f'大於 {threshold}', f'小於等於 {threshold}']\n",
    "sizes = [greater_count, less_equal_count]\n",
    "\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # 確保圓餅圖是圓形\n",
    "plt.title(f'序列長度大於和小於等於 {threshold} 的比例')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12021, 100, 5), (12021, 100))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_seq_to_same_length(datas: list, labels: list, length: int):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "\n",
    "    for i in range(len(datas)):\n",
    "        data = datas[i]\n",
    "        label = labels[i]\n",
    "        start = 0\n",
    "        end = len(data)\n",
    "        while end - start >= length:\n",
    "            train_data.append(data[start:start + length])\n",
    "            train_label.append(label[start:start + length])\n",
    "            start += length\n",
    "        if start != end:\n",
    "            test_data.append(data[start:start + length])\n",
    "            test_label.append(label[start:start + length])\n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data, dtype=object), np.array(test_label, dtype=object)\n",
    "\n",
    "\n",
    "train_data, train_label, test_data, test_label = slice_seq_to_same_length(data_list, label_list, 100)\n",
    "\n",
    "for data in train_data:\n",
    "    assert len(data) == 100\n",
    "\n",
    "train_data.shape, train_label.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([64, 100, 5]), Batch labels shape: torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 將數據轉換為 PyTorch 張量\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_label_tensor = torch.tensor(train_label, dtype=torch.float32)\n",
    "\n",
    "# 建立 TensorDataset\n",
    "dataset = TensorDataset(train_data_tensor, train_label_tensor)\n",
    "\n",
    "# 設定訓練和驗證集的比例，例如 80% 給訓練集，20% 給驗證集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 建立 DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 檢查形狀\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([52, 5]), Label shape: torch.Size([52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiso131\\AppData\\Local\\Temp\\ipykernel_26064\\2696224944.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n",
      "C:\\Users\\weiso131\\AppData\\Local\\Temp\\ipykernel_26064\\2696224944.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_label_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_label]\n"
     ]
    }
   ],
   "source": [
    "# 將數據轉換為 PyTorch 張量列表，保留每個序列的不同長度\n",
    "test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n",
    "test_label_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_label]\n",
    "\n",
    "# 建立自定義 Dataset 用於處理不同長度的序列\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 建立 Test Dataset 和 DataLoader\n",
    "test_dataset = TestDataset(test_data_tensors, test_label_tensors)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 檢查測試資料加載情況\n",
    "for data, label in test_loader:\n",
    "    print(f\"Data shape: {data[0].shape}, Label shape: {label[0].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, tagset_size, input_dim=5):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # LSTM層，輸入維度為 input_dim，輸出維度為 hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=3, batch_first=True)\n",
    "\n",
    "        # 線性層將 LSTM 的輸出映射到標籤空間\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # 初始化隱藏狀態和細胞狀態\n",
    "        return (torch.zeros(3, batch_size, self.hidden_dim),\n",
    "                torch.zeros(3, batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence, hidden):\n",
    "        # sentence 的形狀為 (batch_size, seq_len, input_dim)\n",
    "        # LSTM 層的輸出 lstm_out 形狀為 (batch_size, seq_len, hidden_dim)\n",
    "        # 並傳回更新後的隱藏狀態\n",
    "        lstm_out, hidden = self.lstm(sentence, hidden)\n",
    "\n",
    "        # 使用線性層將 LSTM 的輸出映射到標籤空間\n",
    "        tag_space = self.linear(lstm_out)\n",
    "\n",
    "        # tag_space 的形狀為 (batch_size, seq_len, tagset_size)\n",
    "        return tag_space, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# 設置 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義訓練函數\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10, learning_rate=0.001):\n",
    "    # 將模型移到 GPU\n",
    "    model = model.to(device, dtype=torch.float32)\n",
    "    # 使用 Adam 優化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # 定義損失函數\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # 將輸入和標籤移到 GPU\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = model.init_hidden(batch_size=inputs.size(0))\n",
    "            hidden = tuple([h.to(device, dtype=torch.float32) for h in hidden])\n",
    "\n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 前向傳播\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            # 計算損失\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            # 反向傳播\n",
    "            loss.backward()\n",
    "            # 更新參數\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # 驗證模型\n",
    "        valid_loss = validate_model(model, valid_loader, criterion)\n",
    "\n",
    "# 定義驗證函數\n",
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    error = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            # 將輸入和標籤移到 GPU\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = model.init_hidden(batch_size=inputs.size(0))\n",
    "            hidden = tuple([h.to(device, dtype=torch.float32) for h in hidden])\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            # 計算損失\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            error += abs(outputs.view(-1) - labels.view(-1)).sum()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss / len(valid_loader):.4f}, valid error: {error}\")\n",
    "    return total_loss\n",
    "\n",
    "# 定義測試函數\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # 將輸入和標籤移到 GPU\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = model.init_hidden(batch_size=inputs.size(0))\n",
    "            hidden = tuple([h.to(device, dtype=torch.float32) for h in hidden])\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(labels.cpu().numpy())\n",
    "\n",
    "    return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 305368.4078\n",
      "Validation Loss: 287895.0789, valid error: 29020684.0\n",
      "Epoch [2/500], Training Loss: 294141.6795\n",
      "Validation Loss: 278162.4153, valid error: 28142190.0\n",
      "Epoch [3/500], Training Loss: 284127.9766\n",
      "Validation Loss: 269002.1883, valid error: 27175544.0\n",
      "Epoch [4/500], Training Loss: 274526.8146\n",
      "Validation Loss: 259954.7389, valid error: 25785948.0\n",
      "Epoch [5/500], Training Loss: 265336.2850\n",
      "Validation Loss: 251527.9544, valid error: 25096930.0\n",
      "Epoch [6/500], Training Loss: 256619.8819\n",
      "Validation Loss: 243509.8845, valid error: 24532542.0\n",
      "Epoch [7/500], Training Loss: 248283.4917\n",
      "Validation Loss: 235793.7418, valid error: 23902722.0\n",
      "Epoch [8/500], Training Loss: 242000.7733\n",
      "Validation Loss: 228464.4182, valid error: 23328820.0\n",
      "Epoch [9/500], Training Loss: 232488.1081\n",
      "Validation Loss: 221193.7426, valid error: 22957848.0\n",
      "Epoch [10/500], Training Loss: 225007.7629\n",
      "Validation Loss: 214173.2171, valid error: 22410406.0\n",
      "Epoch [11/500], Training Loss: 217832.9449\n",
      "Validation Loss: 207536.9782, valid error: 22105256.0\n",
      "Epoch [12/500], Training Loss: 211612.2132\n",
      "Validation Loss: 200976.4815, valid error: 21521062.0\n",
      "Epoch [13/500], Training Loss: 204941.8228\n",
      "Validation Loss: 194608.6772, valid error: 20948084.0\n",
      "Epoch [14/500], Training Loss: 198543.3669\n",
      "Validation Loss: 188447.1620, valid error: 20651124.0\n",
      "Epoch [15/500], Training Loss: 190938.0997\n",
      "Validation Loss: 182472.1365, valid error: 20118694.0\n",
      "Epoch [16/500], Training Loss: 185146.8021\n",
      "Validation Loss: 176701.5173, valid error: 19751812.0\n",
      "Epoch [17/500], Training Loss: 178706.8510\n",
      "Validation Loss: 171040.2738, valid error: 19471328.0\n",
      "Epoch [18/500], Training Loss: 172848.8680\n",
      "Validation Loss: 165970.7549, valid error: 19049710.0\n",
      "Epoch [19/500], Training Loss: 167291.4559\n",
      "Validation Loss: 160309.6118, valid error: 18707682.0\n",
      "Epoch [20/500], Training Loss: 162136.2489\n",
      "Validation Loss: 155127.5049, valid error: 18430092.0\n",
      "Epoch [21/500], Training Loss: 156206.2932\n",
      "Validation Loss: 150055.9683, valid error: 17904282.0\n",
      "Epoch [22/500], Training Loss: 150978.1808\n",
      "Validation Loss: 145656.9270, valid error: 18001274.0\n",
      "Epoch [23/500], Training Loss: 146034.9790\n",
      "Validation Loss: 140277.7161, valid error: 17203448.0\n",
      "Epoch [24/500], Training Loss: 140838.3998\n",
      "Validation Loss: 135571.2870, valid error: 16887884.0\n",
      "Epoch [25/500], Training Loss: 135981.7803\n",
      "Validation Loss: 131207.9681, valid error: 16631540.0\n",
      "Epoch [26/500], Training Loss: 134343.8725\n",
      "Validation Loss: 126552.3705, valid error: 16237940.0\n",
      "Epoch [27/500], Training Loss: 126673.4399\n",
      "Validation Loss: 122356.4799, valid error: 15943829.0\n",
      "Epoch [28/500], Training Loss: 124858.3606\n",
      "Validation Loss: 118364.4100, valid error: 15771247.0\n",
      "Epoch [29/500], Training Loss: 118589.9097\n",
      "Validation Loss: 114188.2087, valid error: 15398821.0\n",
      "Epoch [30/500], Training Loss: 113790.0859\n",
      "Validation Loss: 110261.3676, valid error: 14981407.0\n",
      "Epoch [31/500], Training Loss: 109835.2527\n",
      "Validation Loss: 106320.2358, valid error: 14744529.0\n",
      "Epoch [32/500], Training Loss: 105760.4867\n",
      "Validation Loss: 102660.6258, valid error: 14589845.0\n",
      "Epoch [33/500], Training Loss: 103064.7356\n",
      "Validation Loss: 99012.7971, valid error: 14286942.0\n",
      "Epoch [34/500], Training Loss: 98299.4792\n",
      "Validation Loss: 95332.4690, valid error: 13872664.0\n",
      "Epoch [35/500], Training Loss: 95567.5418\n",
      "Validation Loss: 91897.5903, valid error: 13635758.0\n",
      "Epoch [36/500], Training Loss: 91144.3590\n",
      "Validation Loss: 88597.0742, valid error: 13463765.0\n",
      "Epoch [37/500], Training Loss: 89298.1199\n",
      "Validation Loss: 85354.0502, valid error: 13131502.0\n",
      "Epoch [38/500], Training Loss: 84856.4401\n",
      "Validation Loss: 82179.0594, valid error: 12803159.0\n",
      "Epoch [39/500], Training Loss: 81310.2903\n",
      "Validation Loss: 79202.8663, valid error: 12675859.0\n",
      "Epoch [40/500], Training Loss: 78084.4210\n",
      "Validation Loss: 76122.9677, valid error: 12335420.0\n",
      "Epoch [41/500], Training Loss: 77207.6292\n",
      "Validation Loss: 73446.5816, valid error: 12291096.0\n",
      "Epoch [42/500], Training Loss: 73826.0942\n",
      "Validation Loss: 70348.0491, valid error: 11931094.0\n",
      "Epoch [43/500], Training Loss: 69549.5291\n",
      "Validation Loss: 67826.7110, valid error: 11795687.0\n",
      "Epoch [44/500], Training Loss: 66901.4304\n",
      "Validation Loss: 65405.3712, valid error: 11448514.0\n",
      "Epoch [45/500], Training Loss: 64004.7095\n",
      "Validation Loss: 62557.6204, valid error: 11231054.0\n",
      "Epoch [46/500], Training Loss: 61486.1282\n",
      "Validation Loss: 60271.8912, valid error: 10983410.0\n",
      "Epoch [47/500], Training Loss: 60466.8285\n",
      "Validation Loss: 57926.6747, valid error: 10838967.0\n",
      "Epoch [48/500], Training Loss: 56926.1931\n",
      "Validation Loss: 56017.7837, valid error: 10537995.0\n",
      "Epoch [49/500], Training Loss: 54739.3609\n",
      "Validation Loss: 53479.4213, valid error: 10439953.0\n",
      "Epoch [50/500], Training Loss: 52337.3058\n",
      "Validation Loss: 51502.6717, valid error: 10252330.0\n",
      "Epoch [51/500], Training Loss: 50195.8240\n",
      "Validation Loss: 49404.5488, valid error: 10046175.0\n",
      "Epoch [52/500], Training Loss: 48283.4481\n",
      "Validation Loss: 47497.0694, valid error: 9836647.0\n",
      "Epoch [53/500], Training Loss: 47294.3431\n",
      "Validation Loss: 45635.4650, valid error: 9616815.0\n",
      "Epoch [54/500], Training Loss: 44163.4988\n",
      "Validation Loss: 43693.6205, valid error: 9348027.0\n",
      "Epoch [55/500], Training Loss: 42918.7993\n",
      "Validation Loss: 42288.1584, valid error: 9241881.0\n",
      "Epoch [56/500], Training Loss: 41248.6937\n",
      "Validation Loss: 40416.5075, valid error: 9058391.0\n",
      "Epoch [57/500], Training Loss: 39333.4311\n",
      "Validation Loss: 38920.9568, valid error: 8912708.0\n",
      "Epoch [58/500], Training Loss: 37773.9292\n",
      "Validation Loss: 37233.0082, valid error: 8716334.0\n",
      "Epoch [59/500], Training Loss: 35849.5616\n",
      "Validation Loss: 35876.2048, valid error: 8618818.0\n",
      "Epoch [60/500], Training Loss: 35563.8445\n",
      "Validation Loss: 35664.8403, valid error: 8691821.0\n",
      "Epoch [61/500], Training Loss: 33450.5706\n",
      "Validation Loss: 33429.8966, valid error: 8338704.5\n",
      "Epoch [62/500], Training Loss: 31683.8035\n",
      "Validation Loss: 31647.4090, valid error: 7991405.0\n",
      "Epoch [63/500], Training Loss: 30513.2373\n",
      "Validation Loss: 30961.5686, valid error: 8005997.5\n",
      "Epoch [64/500], Training Loss: 29415.3697\n",
      "Validation Loss: 29466.3817, valid error: 7722779.5\n",
      "Epoch [65/500], Training Loss: 28378.0767\n",
      "Validation Loss: 28434.6226, valid error: 7651818.0\n",
      "Epoch [66/500], Training Loss: 27145.3522\n",
      "Validation Loss: 27429.1628, valid error: 7556966.0\n",
      "Epoch [67/500], Training Loss: 26341.7614\n",
      "Validation Loss: 26166.6206, valid error: 7517939.5\n",
      "Epoch [68/500], Training Loss: 24704.4769\n",
      "Validation Loss: 25828.4732, valid error: 7461885.5\n",
      "Epoch [69/500], Training Loss: 24195.1441\n",
      "Validation Loss: 24835.5354, valid error: 7171118.5\n",
      "Epoch [70/500], Training Loss: 23088.5252\n",
      "Validation Loss: 23813.1858, valid error: 6992546.5\n",
      "Epoch [71/500], Training Loss: 22333.5902\n",
      "Validation Loss: 22964.2302, valid error: 6831699.0\n",
      "Epoch [72/500], Training Loss: 20842.0138\n",
      "Validation Loss: 21780.9725, valid error: 6611492.0\n",
      "Epoch [73/500], Training Loss: 20780.8641\n",
      "Validation Loss: 21404.4851, valid error: 6565025.5\n",
      "Epoch [74/500], Training Loss: 19725.6139\n",
      "Validation Loss: 20423.4002, valid error: 6397529.0\n",
      "Epoch [75/500], Training Loss: 19166.5454\n",
      "Validation Loss: 21549.1544, valid error: 6646270.5\n",
      "Epoch [76/500], Training Loss: 18221.9531\n",
      "Validation Loss: 19328.5077, valid error: 6233839.0\n",
      "Epoch [77/500], Training Loss: 17519.7885\n",
      "Validation Loss: 18928.6340, valid error: 6253127.5\n",
      "Epoch [78/500], Training Loss: 16986.5386\n",
      "Validation Loss: 18745.3377, valid error: 6195644.0\n",
      "Epoch [79/500], Training Loss: 16718.1523\n",
      "Validation Loss: 19333.8783, valid error: 6311826.5\n",
      "Epoch [80/500], Training Loss: 16820.6233\n",
      "Validation Loss: 17666.1560, valid error: 6037241.0\n",
      "Epoch [81/500], Training Loss: 15329.9869\n",
      "Validation Loss: 17654.1197, valid error: 5934537.0\n",
      "Epoch [82/500], Training Loss: 15020.7354\n",
      "Validation Loss: 17926.4993, valid error: 6047174.5\n",
      "Epoch [83/500], Training Loss: 14942.1121\n",
      "Validation Loss: 16337.3940, valid error: 5786297.5\n",
      "Epoch [84/500], Training Loss: 13909.3753\n",
      "Validation Loss: 16264.1611, valid error: 5685969.5\n",
      "Epoch [85/500], Training Loss: 14588.7363\n",
      "Validation Loss: 16140.4701, valid error: 5643816.5\n",
      "Epoch [86/500], Training Loss: 13332.1384\n",
      "Validation Loss: 15663.5246, valid error: 5532271.0\n",
      "Epoch [87/500], Training Loss: 12596.3137\n",
      "Validation Loss: 15123.0793, valid error: 5481946.0\n",
      "Epoch [88/500], Training Loss: 12284.0835\n",
      "Validation Loss: 15128.8375, valid error: 5589425.0\n",
      "Epoch [89/500], Training Loss: 12476.9191\n",
      "Validation Loss: 15721.1328, valid error: 5676589.5\n",
      "Epoch [90/500], Training Loss: 13303.1847\n",
      "Validation Loss: 14956.8331, valid error: 5435820.5\n",
      "Epoch [91/500], Training Loss: 11818.7592\n",
      "Validation Loss: 14647.7091, valid error: 5359272.5\n",
      "Epoch [92/500], Training Loss: 11310.3575\n",
      "Validation Loss: 14538.1332, valid error: 5356456.0\n",
      "Epoch [93/500], Training Loss: 10769.7953\n",
      "Validation Loss: 14632.4984, valid error: 5369399.5\n",
      "Epoch [94/500], Training Loss: 10580.2279\n",
      "Validation Loss: 13736.1901, valid error: 5289080.5\n",
      "Epoch [95/500], Training Loss: 10058.0326\n",
      "Validation Loss: 13930.4374, valid error: 5227202.0\n",
      "Epoch [96/500], Training Loss: 9832.0432\n",
      "Validation Loss: 14430.6876, valid error: 5291706.5\n",
      "Epoch [97/500], Training Loss: 9783.8539\n",
      "Validation Loss: 13589.6804, valid error: 5126533.0\n",
      "Epoch [98/500], Training Loss: 9522.4155\n",
      "Validation Loss: 13800.0707, valid error: 5121574.5\n",
      "Epoch [99/500], Training Loss: 9314.7013\n",
      "Validation Loss: 13915.9823, valid error: 5123784.5\n",
      "Epoch [100/500], Training Loss: 10246.5900\n",
      "Validation Loss: 13267.0502, valid error: 5009257.5\n",
      "Epoch [101/500], Training Loss: 9425.0621\n",
      "Validation Loss: 13441.6509, valid error: 5097411.0\n",
      "Epoch [102/500], Training Loss: 8707.7699\n",
      "Validation Loss: 13741.5065, valid error: 5079106.5\n",
      "Epoch [103/500], Training Loss: 8698.7987\n",
      "Validation Loss: 13033.1177, valid error: 4974207.0\n",
      "Epoch [104/500], Training Loss: 8599.9473\n",
      "Validation Loss: 12986.3844, valid error: 5012351.0\n",
      "Epoch [105/500], Training Loss: 8233.4554\n",
      "Validation Loss: 13421.8785, valid error: 5058704.5\n",
      "Epoch [106/500], Training Loss: 8346.6266\n",
      "Validation Loss: 13018.7769, valid error: 4923645.5\n",
      "Epoch [107/500], Training Loss: 7694.1964\n",
      "Validation Loss: 12915.0688, valid error: 4912782.5\n",
      "Epoch [108/500], Training Loss: 7904.4863\n",
      "Validation Loss: 13182.5087, valid error: 5011890.0\n",
      "Epoch [109/500], Training Loss: 7944.6034\n",
      "Validation Loss: 13222.5621, valid error: 5046601.5\n",
      "Epoch [110/500], Training Loss: 7762.8696\n",
      "Validation Loss: 12423.6114, valid error: 4901607.0\n",
      "Epoch [111/500], Training Loss: 7780.3504\n",
      "Validation Loss: 12424.3917, valid error: 4828099.5\n",
      "Epoch [112/500], Training Loss: 7465.3681\n",
      "Validation Loss: 12492.1301, valid error: 4836051.5\n",
      "Epoch [113/500], Training Loss: 7855.1305\n",
      "Validation Loss: 11984.3559, valid error: 4783264.5\n",
      "Epoch [114/500], Training Loss: 7135.4969\n",
      "Validation Loss: 12138.4128, valid error: 4818453.5\n",
      "Epoch [115/500], Training Loss: 6999.3594\n",
      "Validation Loss: 11719.1555, valid error: 4698975.0\n",
      "Epoch [116/500], Training Loss: 6902.8327\n",
      "Validation Loss: 11814.8858, valid error: 4703603.5\n",
      "Epoch [117/500], Training Loss: 6495.9591\n",
      "Validation Loss: 11733.8723, valid error: 4746954.0\n",
      "Epoch [118/500], Training Loss: 6381.3765\n",
      "Validation Loss: 11721.4163, valid error: 4645628.5\n",
      "Epoch [119/500], Training Loss: 6364.6095\n",
      "Validation Loss: 11688.8323, valid error: 4677560.0\n",
      "Epoch [120/500], Training Loss: 6603.0216\n",
      "Validation Loss: 12574.2398, valid error: 4946292.0\n",
      "Epoch [121/500], Training Loss: 6712.8118\n",
      "Validation Loss: 11763.3320, valid error: 4746659.0\n",
      "Epoch [122/500], Training Loss: 6398.0777\n",
      "Validation Loss: 12084.1272, valid error: 4813248.5\n",
      "Epoch [123/500], Training Loss: 6712.7505\n",
      "Validation Loss: 12534.2260, valid error: 4829634.0\n",
      "Epoch [124/500], Training Loss: 7042.6429\n",
      "Validation Loss: 12662.2176, valid error: 5007538.5\n",
      "Epoch [125/500], Training Loss: 6509.9065\n",
      "Validation Loss: 11871.3872, valid error: 4713789.5\n",
      "Epoch [126/500], Training Loss: 5798.1119\n",
      "Validation Loss: 11540.8170, valid error: 4599738.0\n",
      "Epoch [127/500], Training Loss: 5772.9708\n",
      "Validation Loss: 12140.6293, valid error: 4805838.0\n",
      "Epoch [128/500], Training Loss: 5778.2033\n",
      "Validation Loss: 11590.9109, valid error: 4596908.5\n",
      "Epoch [129/500], Training Loss: 5732.7605\n",
      "Validation Loss: 12532.4194, valid error: 4914009.0\n",
      "Epoch [130/500], Training Loss: 8723.6941\n",
      "Validation Loss: 12452.0796, valid error: 4860478.0\n",
      "Epoch [131/500], Training Loss: 7421.3996\n",
      "Validation Loss: 12691.0661, valid error: 4823799.5\n",
      "Epoch [132/500], Training Loss: 6080.0386\n",
      "Validation Loss: 11809.2199, valid error: 4628753.5\n",
      "Epoch [133/500], Training Loss: 5458.4454\n",
      "Validation Loss: 11732.5830, valid error: 4660965.0\n",
      "Epoch [134/500], Training Loss: 5234.4383\n",
      "Validation Loss: 11149.4680, valid error: 4515434.5\n",
      "Epoch [135/500], Training Loss: 5625.8296\n",
      "Validation Loss: 11681.0206, valid error: 4628973.0\n",
      "Epoch [136/500], Training Loss: 5338.7018\n",
      "Validation Loss: 11132.0742, valid error: 4501576.5\n",
      "Epoch [137/500], Training Loss: 5207.5348\n",
      "Validation Loss: 12078.0984, valid error: 4716444.5\n",
      "Epoch [138/500], Training Loss: 5500.6956\n",
      "Validation Loss: 12003.9966, valid error: 4611146.5\n",
      "Epoch [139/500], Training Loss: 5269.5125\n",
      "Validation Loss: 11598.3616, valid error: 4593713.0\n",
      "Epoch [140/500], Training Loss: 5057.2195\n",
      "Validation Loss: 11344.4267, valid error: 4544576.0\n",
      "Epoch [141/500], Training Loss: 5010.3416\n",
      "Validation Loss: 11535.5877, valid error: 4652896.5\n",
      "Epoch [142/500], Training Loss: 4758.0296\n",
      "Validation Loss: 11612.1029, valid error: 4636207.5\n",
      "Epoch [143/500], Training Loss: 4639.5975\n",
      "Validation Loss: 10760.5900, valid error: 4460569.5\n",
      "Epoch [144/500], Training Loss: 4481.6859\n",
      "Validation Loss: 11605.9523, valid error: 4653075.0\n",
      "Epoch [145/500], Training Loss: 5220.6033\n",
      "Validation Loss: 12036.9168, valid error: 4696158.5\n",
      "Epoch [146/500], Training Loss: 4985.2842\n",
      "Validation Loss: 11263.0244, valid error: 4521464.5\n",
      "Epoch [147/500], Training Loss: 5743.5387\n",
      "Validation Loss: 11390.0350, valid error: 4656782.5\n",
      "Epoch [148/500], Training Loss: 4694.7886\n",
      "Validation Loss: 11232.0319, valid error: 4537947.0\n",
      "Epoch [149/500], Training Loss: 4490.8512\n",
      "Validation Loss: 11246.3290, valid error: 4528927.5\n",
      "Epoch [150/500], Training Loss: 4305.5315\n",
      "Validation Loss: 11509.1403, valid error: 4580729.5\n",
      "Epoch [151/500], Training Loss: 4440.8209\n",
      "Validation Loss: 11594.2492, valid error: 4568252.5\n",
      "Epoch [152/500], Training Loss: 4488.4005\n",
      "Validation Loss: 11537.4069, valid error: 4611747.0\n",
      "Epoch [153/500], Training Loss: 4333.1001\n",
      "Validation Loss: 11347.7807, valid error: 4569337.0\n",
      "Epoch [154/500], Training Loss: 4726.6164\n",
      "Validation Loss: 11374.9858, valid error: 4556819.0\n",
      "Epoch [155/500], Training Loss: 4823.8497\n",
      "Validation Loss: 11625.0855, valid error: 4631864.5\n",
      "Epoch [156/500], Training Loss: 4356.9358\n",
      "Validation Loss: 11436.0862, valid error: 4582322.0\n",
      "Epoch [157/500], Training Loss: 4525.9678\n",
      "Validation Loss: 11735.6073, valid error: 4592391.5\n",
      "Epoch [158/500], Training Loss: 4279.2811\n",
      "Validation Loss: 11640.7210, valid error: 4565914.0\n",
      "Epoch [159/500], Training Loss: 4263.4461\n",
      "Validation Loss: 11786.0379, valid error: 4541884.5\n",
      "Epoch [160/500], Training Loss: 3980.5237\n",
      "Validation Loss: 12020.9202, valid error: 4598330.5\n",
      "Epoch [161/500], Training Loss: 4905.0116\n",
      "Validation Loss: 12719.1863, valid error: 4871681.0\n",
      "Epoch [162/500], Training Loss: 5335.7713\n",
      "Validation Loss: 12081.7930, valid error: 4662600.0\n",
      "Epoch [163/500], Training Loss: 4532.9445\n",
      "Validation Loss: 11430.3789, valid error: 4518156.0\n",
      "Epoch [164/500], Training Loss: 3965.4742\n",
      "Validation Loss: 11325.5624, valid error: 4498571.5\n",
      "Epoch [165/500], Training Loss: 4130.2946\n",
      "Validation Loss: 11380.3907, valid error: 4516209.5\n",
      "Epoch [166/500], Training Loss: 3882.3098\n",
      "Validation Loss: 12312.6850, valid error: 4649097.0\n",
      "Epoch [167/500], Training Loss: 3852.3185\n",
      "Validation Loss: 11511.9302, valid error: 4534423.0\n",
      "Epoch [168/500], Training Loss: 3746.1629\n",
      "Validation Loss: 11976.2332, valid error: 4582165.0\n",
      "Epoch [169/500], Training Loss: 3598.8950\n",
      "Validation Loss: 11673.6345, valid error: 4612638.5\n",
      "Epoch [170/500], Training Loss: 3549.5159\n",
      "Validation Loss: 12151.5966, valid error: 4610028.5\n",
      "Epoch [171/500], Training Loss: 3835.1357\n",
      "Validation Loss: 11513.0688, valid error: 4572513.5\n",
      "Epoch [172/500], Training Loss: 3757.3075\n",
      "Validation Loss: 11788.9668, valid error: 4578416.0\n",
      "Epoch [173/500], Training Loss: 4615.7099\n",
      "Validation Loss: 11238.3292, valid error: 4533944.5\n",
      "Epoch [174/500], Training Loss: 3819.5249\n",
      "Validation Loss: 11214.7439, valid error: 4497975.0\n",
      "Epoch [175/500], Training Loss: 4679.1786\n",
      "Validation Loss: 11943.1462, valid error: 4653648.0\n",
      "Epoch [176/500], Training Loss: 3989.7423\n",
      "Validation Loss: 11918.2776, valid error: 4567585.0\n",
      "Epoch [177/500], Training Loss: 3660.4181\n",
      "Validation Loss: 11878.6953, valid error: 4591301.0\n",
      "Epoch [178/500], Training Loss: 3640.7417\n",
      "Validation Loss: 11473.3877, valid error: 4536775.5\n",
      "Epoch [179/500], Training Loss: 3473.4715\n",
      "Validation Loss: 11614.7731, valid error: 4543020.5\n",
      "Epoch [180/500], Training Loss: 3256.1384\n",
      "Validation Loss: 12068.0488, valid error: 4554407.5\n",
      "Epoch [181/500], Training Loss: 3452.2760\n",
      "Validation Loss: 11713.6831, valid error: 4535552.0\n",
      "Epoch [182/500], Training Loss: 3647.5595\n",
      "Validation Loss: 11903.1582, valid error: 4547283.0\n",
      "Epoch [183/500], Training Loss: 3678.0368\n",
      "Validation Loss: 11896.0553, valid error: 4764833.0\n",
      "Epoch [184/500], Training Loss: 3683.6052\n",
      "Validation Loss: 11811.0887, valid error: 4609772.5\n",
      "Epoch [185/500], Training Loss: 3355.1904\n",
      "Validation Loss: 11691.9093, valid error: 4578025.5\n",
      "Epoch [186/500], Training Loss: 3160.0405\n",
      "Validation Loss: 11720.1375, valid error: 4525568.5\n",
      "Epoch [187/500], Training Loss: 3027.7378\n",
      "Validation Loss: 11679.1149, valid error: 4568467.0\n",
      "Epoch [188/500], Training Loss: 3078.4015\n",
      "Validation Loss: 11566.5971, valid error: 4528485.0\n",
      "Epoch [189/500], Training Loss: 3312.7012\n",
      "Validation Loss: 11908.6748, valid error: 4572464.0\n",
      "Epoch [190/500], Training Loss: 3575.5766\n",
      "Validation Loss: 11488.9312, valid error: 4547846.5\n",
      "Epoch [191/500], Training Loss: 3032.0972\n",
      "Validation Loss: 12221.7904, valid error: 4602477.5\n",
      "Epoch [192/500], Training Loss: 3228.0662\n",
      "Validation Loss: 11041.1806, valid error: 4452871.5\n",
      "Epoch [193/500], Training Loss: 3039.1368\n",
      "Validation Loss: 11368.7358, valid error: 4451853.5\n",
      "Epoch [194/500], Training Loss: 3025.5739\n",
      "Validation Loss: 12137.0815, valid error: 4621597.0\n",
      "Epoch [195/500], Training Loss: 3669.0785\n",
      "Validation Loss: 12288.8395, valid error: 4740414.0\n",
      "Epoch [196/500], Training Loss: 3425.1010\n",
      "Validation Loss: 11679.1856, valid error: 4612281.5\n",
      "Epoch [197/500], Training Loss: 3426.4027\n",
      "Validation Loss: 11873.8853, valid error: 4604986.5\n",
      "Epoch [198/500], Training Loss: 2892.8315\n",
      "Validation Loss: 11447.4462, valid error: 4602936.0\n",
      "Epoch [199/500], Training Loss: 3336.7483\n",
      "Validation Loss: 11432.2462, valid error: 4515266.0\n",
      "Epoch [200/500], Training Loss: 3067.5760\n",
      "Validation Loss: 11695.2876, valid error: 4571237.5\n",
      "Epoch [201/500], Training Loss: 2971.1228\n",
      "Validation Loss: 12288.2106, valid error: 4750208.0\n",
      "Epoch [202/500], Training Loss: 3103.1693\n",
      "Validation Loss: 11664.3236, valid error: 4601717.5\n",
      "Epoch [203/500], Training Loss: 3013.5295\n",
      "Validation Loss: 11392.5914, valid error: 4509154.0\n",
      "Epoch [204/500], Training Loss: 3149.8999\n",
      "Validation Loss: 12149.0897, valid error: 4606541.0\n",
      "Epoch [205/500], Training Loss: 2915.4556\n",
      "Validation Loss: 12424.0426, valid error: 4697996.0\n",
      "Epoch [206/500], Training Loss: 3910.8158\n",
      "Validation Loss: 13012.7239, valid error: 5088800.0\n",
      "Epoch [207/500], Training Loss: 4404.6711\n",
      "Validation Loss: 11821.4314, valid error: 4734352.5\n",
      "Epoch [208/500], Training Loss: 4732.8734\n",
      "Validation Loss: 12407.2939, valid error: 4785268.5\n",
      "Epoch [209/500], Training Loss: 4076.3651\n",
      "Validation Loss: 12828.4996, valid error: 4761061.0\n",
      "Epoch [210/500], Training Loss: 3383.5811\n",
      "Validation Loss: 11895.0454, valid error: 4589604.5\n",
      "Epoch [211/500], Training Loss: 3011.0294\n",
      "Validation Loss: 12019.1384, valid error: 4612376.0\n",
      "Epoch [212/500], Training Loss: 2828.0964\n",
      "Validation Loss: 12152.9529, valid error: 4620208.5\n",
      "Epoch [213/500], Training Loss: 3038.4959\n",
      "Validation Loss: 11830.7156, valid error: 4759797.0\n",
      "Epoch [214/500], Training Loss: 4797.7875\n",
      "Validation Loss: 12344.5231, valid error: 4690290.0\n",
      "Epoch [215/500], Training Loss: 3416.1324\n",
      "Validation Loss: 11640.8517, valid error: 4531707.5\n",
      "Epoch [216/500], Training Loss: 4034.9285\n",
      "Validation Loss: 11676.4301, valid error: 4571708.0\n",
      "Epoch [217/500], Training Loss: 3358.4213\n",
      "Validation Loss: 11768.3905, valid error: 4586091.0\n",
      "Epoch [218/500], Training Loss: 2907.5354\n",
      "Validation Loss: 11828.3191, valid error: 4619001.5\n",
      "Epoch [219/500], Training Loss: 2844.3704\n",
      "Validation Loss: 12403.3433, valid error: 4660477.5\n",
      "Epoch [220/500], Training Loss: 2858.7979\n",
      "Validation Loss: 11851.2870, valid error: 4556295.5\n",
      "Epoch [221/500], Training Loss: 2647.7931\n",
      "Validation Loss: 11780.0093, valid error: 4510800.0\n",
      "Epoch [222/500], Training Loss: 2464.9268\n",
      "Validation Loss: 11629.4139, valid error: 4494838.0\n",
      "Epoch [223/500], Training Loss: 2472.5754\n",
      "Validation Loss: 12204.0177, valid error: 4533845.5\n",
      "Epoch [224/500], Training Loss: 2640.4460\n",
      "Validation Loss: 12001.0796, valid error: 4578850.0\n",
      "Epoch [225/500], Training Loss: 2739.1662\n",
      "Validation Loss: 11861.1633, valid error: 4532476.5\n",
      "Epoch [226/500], Training Loss: 2785.3560\n",
      "Validation Loss: 12235.5334, valid error: 4565735.0\n",
      "Epoch [227/500], Training Loss: 2797.7843\n",
      "Validation Loss: 11754.2014, valid error: 4635115.0\n",
      "Epoch [228/500], Training Loss: 2771.9293\n",
      "Validation Loss: 11755.7870, valid error: 4561560.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMTagger(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 將輸入和標籤移到 GPU\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# 初始化隱藏狀態\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(128, 1)\n",
    "train_model(model, train_loader, valid_loader, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 52])) that is different to the input size (torch.Size([52])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 18])) that is different to the input size (torch.Size([18])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 27])) that is different to the input size (torch.Size([27])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 33])) that is different to the input size (torch.Size([33])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 29])) that is different to the input size (torch.Size([29])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 72])) that is different to the input size (torch.Size([72])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 73])) that is different to the input size (torch.Size([73])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 26])) that is different to the input size (torch.Size([26])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 80])) that is different to the input size (torch.Size([80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 31])) that is different to the input size (torch.Size([31])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 38])) that is different to the input size (torch.Size([38])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 36])) that is different to the input size (torch.Size([36])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 34])) that is different to the input size (torch.Size([34])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 43])) that is different to the input size (torch.Size([43])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 93])) that is different to the input size (torch.Size([93])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 48])) that is different to the input size (torch.Size([48])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 39])) that is different to the input size (torch.Size([39])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 42])) that is different to the input size (torch.Size([42])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 49])) that is different to the input size (torch.Size([49])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 44])) that is different to the input size (torch.Size([44])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 50])) that is different to the input size (torch.Size([50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 98])) that is different to the input size (torch.Size([98])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 53])) that is different to the input size (torch.Size([53])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 62])) that is different to the input size (torch.Size([62])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 54])) that is different to the input size (torch.Size([54])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 58])) that is different to the input size (torch.Size([58])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 35])) that is different to the input size (torch.Size([35])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 70])) that is different to the input size (torch.Size([70])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 59])) that is different to the input size (torch.Size([59])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 90])) that is different to the input size (torch.Size([90])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 63])) that is different to the input size (torch.Size([63])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 64])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 81])) that is different to the input size (torch.Size([81])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 16])) that is different to the input size (torch.Size([16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 76])) that is different to the input size (torch.Size([76])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 84])) that is different to the input size (torch.Size([84])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 79])) that is different to the input size (torch.Size([79])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 17])) that is different to the input size (torch.Size([17])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 89])) that is different to the input size (torch.Size([89])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 94])) that is different to the input size (torch.Size([94])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 22])) that is different to the input size (torch.Size([22])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 15])) that is different to the input size (torch.Size([15])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 95])) that is different to the input size (torch.Size([95])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 92])) that is different to the input size (torch.Size([92])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 99])) that is different to the input size (torch.Size([99])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 96])) that is different to the input size (torch.Size([96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 97])) that is different to the input size (torch.Size([97])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 10])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 11])) that is different to the input size (torch.Size([11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 7])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 14])) that is different to the input size (torch.Size([14])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 20])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 19])) that is different to the input size (torch.Size([19])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 37])) that is different to the input size (torch.Size([37])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 9])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 91])) that is different to the input size (torch.Size([91])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 24])) that is different to the input size (torch.Size([24])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 41])) that is different to the input size (torch.Size([41])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 25])) that is different to the input size (torch.Size([25])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 30])) that is different to the input size (torch.Size([30])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 71])) that is different to the input size (torch.Size([71])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 12])) that is different to the input size (torch.Size([12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 74])) that is different to the input size (torch.Size([74])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 32])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 40])) that is different to the input size (torch.Size([40])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 28])) that is different to the input size (torch.Size([28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 21])) that is different to the input size (torch.Size([21])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 13])) that is different to the input size (torch.Size([13])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 5])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 23])) that is different to the input size (torch.Size([23])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 47])) that is different to the input size (torch.Size([47])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 87])) that is different to the input size (torch.Size([87])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 46])) that is different to the input size (torch.Size([46])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 61])) that is different to the input size (torch.Size([61])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 51])) that is different to the input size (torch.Size([51])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 68])) that is different to the input size (torch.Size([68])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 57])) that is different to the input size (torch.Size([57])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 82])) that is different to the input size (torch.Size([82])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 75])) that is different to the input size (torch.Size([75])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 78])) that is different to the input size (torch.Size([78])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 67])) that is different to the input size (torch.Size([67])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 83])) that is different to the input size (torch.Size([83])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 77])) that is different to the input size (torch.Size([77])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 88])) that is different to the input size (torch.Size([88])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 60])) that is different to the input size (torch.Size([60])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 55])) that is different to the input size (torch.Size([55])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 56])) that is different to the input size (torch.Size([56])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 85])) that is different to the input size (torch.Size([85])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 65])) that is different to the input size (torch.Size([65])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 69])) that is different to the input size (torch.Size([69])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 86])) that is different to the input size (torch.Size([86])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 66])) that is different to the input size (torch.Size([66])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 45])) that is different to the input size (torch.Size([45])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3153.9061, valid error: 820247.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6222656.732362235"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "validate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88152"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num = 0\n",
    "for i in range(len(test_data)):\n",
    "    data_num += len(test_data[i])\n",
    "data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
