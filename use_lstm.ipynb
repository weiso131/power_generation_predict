{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationCode</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>WindSpeed(m/s)</th>\n",
       "      <th>Pressure(hpa)</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Sunlight(Lux)</th>\n",
       "      <th>Power(mW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:14:06.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>15.59</td>\n",
       "      <td>94.30</td>\n",
       "      <td>652.92</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:14:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>15.66</td>\n",
       "      <td>94.04</td>\n",
       "      <td>682.50</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:15:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.47</td>\n",
       "      <td>15.74</td>\n",
       "      <td>94.10</td>\n",
       "      <td>750.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:16:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.46</td>\n",
       "      <td>15.78</td>\n",
       "      <td>94.09</td>\n",
       "      <td>738.33</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2024-03-01 17:17:47.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.49</td>\n",
       "      <td>15.80</td>\n",
       "      <td>94.08</td>\n",
       "      <td>660.83</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89607</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:50:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.54</td>\n",
       "      <td>30.69</td>\n",
       "      <td>72.91</td>\n",
       "      <td>2288.33</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89608</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:51:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.40</td>\n",
       "      <td>30.27</td>\n",
       "      <td>73.16</td>\n",
       "      <td>3236.67</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89609</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:52:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.39</td>\n",
       "      <td>29.90</td>\n",
       "      <td>72.51</td>\n",
       "      <td>4526.67</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89610</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:53:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.40</td>\n",
       "      <td>29.38</td>\n",
       "      <td>73.23</td>\n",
       "      <td>4231.67</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89611</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-07-23 15:54:57.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>994.58</td>\n",
       "      <td>29.06</td>\n",
       "      <td>74.22</td>\n",
       "      <td>3685.83</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290894 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LocationCode                 DateTime  WindSpeed(m/s)  Pressure(hpa)  \\\n",
       "0                10  2024-03-01 17:14:06.000             0.0        1017.48   \n",
       "1                10  2024-03-01 17:14:47.000             0.0        1017.48   \n",
       "2                10  2024-03-01 17:15:47.000             0.0        1017.47   \n",
       "3                10  2024-03-01 17:16:47.000             0.0        1017.46   \n",
       "4                10  2024-03-01 17:17:47.000             0.0        1017.49   \n",
       "...             ...                      ...             ...            ...   \n",
       "89607             9  2024-07-23 15:50:57.000             0.0         994.54   \n",
       "89608             9  2024-07-23 15:51:57.000             0.0         994.40   \n",
       "89609             9  2024-07-23 15:52:57.000             0.0         994.39   \n",
       "89610             9  2024-07-23 15:53:57.000             0.0         994.40   \n",
       "89611             9  2024-07-23 15:54:57.000             0.0         994.58   \n",
       "\n",
       "       Temperature(°C)  Humidity(%)  Sunlight(Lux)  Power(mW)  \n",
       "0                15.59        94.30         652.92       0.12  \n",
       "1                15.66        94.04         682.50       0.12  \n",
       "2                15.74        94.10         750.00       0.14  \n",
       "3                15.78        94.09         738.33       0.14  \n",
       "4                15.80        94.08         660.83       0.12  \n",
       "...                ...          ...            ...        ...  \n",
       "89607            30.69        72.91        2288.33       1.10  \n",
       "89608            30.27        73.16        3236.67       1.92  \n",
       "89609            29.90        72.51        4526.67       3.57  \n",
       "89610            29.38        73.23        4231.67       3.13  \n",
       "89611            29.06        74.22        3685.83       2.46  \n",
       "\n",
       "[1290894 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_list = os.listdir(\"train\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in csv_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_temp = pd.read_csv(f\"train/{file}\")\n",
    "        df = pd.concat([df, df_temp])\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.LeaveOneOutEncoder(cols=[\"LocationCode\"], sigma = 0.05)\n",
    "encoder.fit(df, df['Power(mW)'])\n",
    "df = encoder.transform(df)\n",
    "\n",
    "# 指定要標準化的欄位\n",
    "columns_to_standardize = ['WindSpeed(m/s)', 'Pressure(hpa)', 'Temperature(°C)', 'Humidity(%)', 'Sunlight(Lux)', \"LocationCode\"]\n",
    "\n",
    "# 初始化 StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 對指定欄位進行標準化\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_10min(df):\n",
    "    location = df[\"LocationCode\"].unique()\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for l in location:\n",
    "        l_df = df[df[\"LocationCode\"] == l]\n",
    "        l_df.set_index('DateTime', inplace=True)\n",
    "        l_df = l_df.resample('10min').mean().dropna()  # 將 '10T' 改為 '10min'\n",
    "        l_df = l_df.reset_index()\n",
    "        l_df[\"LocationCode\"] = l  # 添加 LocationCode 列\n",
    "        #l_df = l_df.drop(columns=[\"DateTime\", \"LocationCode\"])  # 如果不需要 DateTime 列的話可以刪除\n",
    "        \n",
    "        # 將 l_df 與 new_df 合併\n",
    "        new_df = pd.concat([new_df, l_df], ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "df = mean_10min(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2024-03-01 17:10:00\n",
       "1        2024-03-01 17:20:00\n",
       "2        2024-03-01 17:30:00\n",
       "3        2024-03-01 17:40:00\n",
       "4        2024-03-01 17:50:00\n",
       "                 ...        \n",
       "131750   2024-07-23 15:10:00\n",
       "131751   2024-07-23 15:20:00\n",
       "131752   2024-07-23 15:30:00\n",
       "131753   2024-07-23 15:40:00\n",
       "131754   2024-07-23 15:50:00\n",
       "Name: DateTime, Length: 131755, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DateTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_data_with_datetime(df: pd.DataFrame):\n",
    "    op_df = list(df['DateTime'].dt.day)\n",
    "    \n",
    "    data_df = df.drop(columns=['DateTime', 'Power(mW)'])\n",
    "    label_df = df['Power(mW)']\n",
    "\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    last_index = 0\n",
    "    for i in range(1, len(op_df) - 1):\n",
    "        if op_df[i] != op_df[i - 1]:\n",
    "            data_list.append(torch.from_numpy(np.array(data_df.iloc[last_index: i])))\n",
    "            label_list.append(torch.from_numpy(np.array(label_df.iloc[last_index:i])))\n",
    "            last_index = i\n",
    "    return data_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 6]), torch.Size([6]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list, label_list = spilt_data_with_datetime(df)\n",
    "\n",
    "data_list[0].shape, label_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24207 (\\N{CJK UNIFIED IDEOGRAPH-5E8F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21015 (\\N{CJK UNIFIED IDEOGRAPH-5217}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 38263 (\\N{CJK UNIFIED IDEOGRAPH-9577}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24230 (\\N{CJK UNIFIED IDEOGRAPH-5EA6}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 22823 (\\N{CJK UNIFIED IDEOGRAPH-5927}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 26044 (\\N{CJK UNIFIED IDEOGRAPH-65BC}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21644 (\\N{CJK UNIFIED IDEOGRAPH-548C}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 23567 (\\N{CJK UNIFIED IDEOGRAPH-5C0F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 31561 (\\N{CJK UNIFIED IDEOGRAPH-7B49}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\weiso131\\anaconda3\\envs\\AI\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 20363 (\\N{CJK UNIFIED IDEOGRAPH-4F8B}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 假設這是你的字串陣列\n",
    "\n",
    "# 計算每個字串的長度\n",
    "lengths = [len(s) for s in data_list]\n",
    "\n",
    "# 設定臨界長度\n",
    "threshold = 10\n",
    "\n",
    "# 計算大於和小於等於 threshold 的比例\n",
    "greater_count = sum(1 for length in lengths if length > threshold)\n",
    "less_equal_count = sum(1 for length in lengths if length <= threshold)\n",
    "\n",
    "# 繪製圓餅圖\n",
    "target = [f'> {threshold}', f'<= {threshold}']\n",
    "sizes = [greater_count, less_equal_count]\n",
    "\n",
    "plt.pie(sizes, labels=target, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # 確保圓餅圖是圓形\n",
    "plt.title(f'序列長度大於和小於等於 {threshold} 的比例')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12309, 10, 6), (12309, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slice_seq_to_same_length(datas: list, labels: list, length: int):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "\n",
    "    for i in range(len(datas)):\n",
    "        data = datas[i]\n",
    "        label = labels[i]\n",
    "        start = 0\n",
    "        end = len(data)\n",
    "        while end - start >= length:\n",
    "            train_data.append(data[start:start + length])\n",
    "            train_label.append(label[start:start + length])\n",
    "            start += length\n",
    "        if start != end:\n",
    "            test_data.append(data[start:start + length])\n",
    "            test_label.append(label[start:start + length])\n",
    "    return np.array(train_data), np.array(train_label), np.array(test_data, dtype=object), np.array(test_label, dtype=object)\n",
    "\n",
    "\n",
    "train_data, train_label, test_data, test_label = slice_seq_to_same_length(data_list, label_list, 10)\n",
    "\n",
    "for data in train_data:\n",
    "    assert len(data) == 10\n",
    "\n",
    "train_data.shape, train_label.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, labels):\n",
    "        self.max_val = labels.max()\n",
    "        self.min_val = labels.min()\n",
    "    def normalize(self, target):\n",
    "        return (target - self.min_val) / (self.max_val - self.min_val)\n",
    "    def denormalize(self, target):\n",
    "        return target * (self.max_val - self.min_val) + self.min_val\n",
    "    \n",
    "label_normalizer = Normalizer(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([64, 10, 6]), Batch labels shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "#train_labels_norm = label_normalizer.normalize(train_label)\n",
    "\n",
    "\n",
    "# 將數據轉換為 PyTorch 張量\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_label_tensor = torch.tensor(train_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# 建立 TensorDataset\n",
    "dataset = TensorDataset(train_data_tensor, train_label_tensor)\n",
    "\n",
    "# 設定訓練和驗證集的比例，例如 80% 給訓練集，20% 給驗證集\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 建立 DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 檢查形狀\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([6, 6]), Label shape: torch.Size([6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiso131\\AppData\\Local\\Temp\\ipykernel_27416\\2696224944.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n",
      "C:\\Users\\weiso131\\AppData\\Local\\Temp\\ipykernel_27416\\2696224944.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_label_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_label]\n"
     ]
    }
   ],
   "source": [
    "# 將數據轉換為 PyTorch 張量列表，保留每個序列的不同長度\n",
    "test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n",
    "test_label_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_label]\n",
    "\n",
    "# 建立自定義 Dataset 用於處理不同長度的序列\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 建立 Test Dataset 和 DataLoader\n",
    "test_dataset = TestDataset(test_data_tensors, test_label_tensors)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 檢查測試資料加載情況\n",
    "for data, label in test_loader:\n",
    "    print(f\"Data shape: {data[0].shape}, Label shape: {label[0].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, tagset_size, input_dim=6):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # LSTM層，輸入維度為 input_dim，輸出維度為 hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # 線性層將 LSTM 的輸出映射到標籤空間\n",
    "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # 初始化隱藏狀態和細胞狀態\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence, hidden):\n",
    "        # sentence 的形狀為 (batch_size, seq_len, input_dim)\n",
    "        # LSTM 層的輸出 lstm_out 形狀為 (batch_size, seq_len, hidden_dim)\n",
    "        # 並傳回更新後的隱藏狀態\n",
    "        lstm_out, hidden = self.lstm(sentence, hidden)\n",
    "\n",
    "        # 使用線性層將 LSTM 的輸出映射到標籤空間\n",
    "        tag_space = self.relu(self.linear(self.relu(lstm_out)))\n",
    "\n",
    "        # tag_space 的形狀為 (batch_size, seq_len, tagset_size)\n",
    "        return tag_space, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# 設置 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義訓練函數\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10, learning_rate=0.001):\n",
    "    # 將模型移到 GPU\n",
    "    model = model.to(device, dtype=torch.float32)\n",
    "    # 使用 Adam 優化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # 定義損失函數\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # 將輸入和標籤移到 GPU\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = model.init_hidden(batch_size=inputs.size(0))\n",
    "            hidden = tuple([h.to(device, dtype=torch.float32) for h in hidden])\n",
    "\n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 前向傳播\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            # 計算損失\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            # 反向傳播\n",
    "            loss.backward()\n",
    "            # 更新參數\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # 驗證模型\n",
    "        valid_loss = validate_model(model, valid_loader, criterion)\n",
    "\n",
    "# 定義驗證函數\n",
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    error = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            # 將輸入和標籤移到 GPU\n",
    "            inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = model.init_hidden(batch_size=inputs.size(0))\n",
    "            hidden = tuple([h.to(device, dtype=torch.float32) for h in hidden])\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            # 計算損失\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            error += abs(outputs.view(-1) - labels.view(-1)).sum() / inputs.shape[0] / inputs.shape[1]\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss / len(valid_loader):.4f}, valid error: {error / len(valid_loader)}\")\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 253.5308\n",
      "Validation Loss: 251.3319, valid error: 251.8185577392578\n",
      "Epoch [2/100], Training Loss: 244.3198\n",
      "Validation Loss: 237.0252, valid error: 237.50498962402344\n",
      "Epoch [3/100], Training Loss: 230.8330\n",
      "Validation Loss: 224.8654, valid error: 225.3321990966797\n",
      "Epoch [4/100], Training Loss: 219.2224\n",
      "Validation Loss: 214.2641, valid error: 214.71572875976562\n",
      "Epoch [5/100], Training Loss: 208.4891\n",
      "Validation Loss: 204.6267, valid error: 205.09397888183594\n",
      "Epoch [6/100], Training Loss: 201.8168\n",
      "Validation Loss: 196.0761, valid error: 196.51744079589844\n",
      "Epoch [7/100], Training Loss: 192.9746\n",
      "Validation Loss: 188.5858, valid error: 189.01638793945312\n",
      "Epoch [8/100], Training Loss: 184.7717\n",
      "Validation Loss: 180.6387, valid error: 181.07936096191406\n",
      "Epoch [9/100], Training Loss: 176.8427\n",
      "Validation Loss: 173.6798, valid error: 174.10862731933594\n",
      "Epoch [10/100], Training Loss: 171.2316\n",
      "Validation Loss: 166.5855, valid error: 167.0147705078125\n",
      "Epoch [11/100], Training Loss: 163.9980\n",
      "Validation Loss: 159.7465, valid error: 160.16624450683594\n",
      "Epoch [12/100], Training Loss: 156.8161\n",
      "Validation Loss: 153.5738, valid error: 153.9979248046875\n",
      "Epoch [13/100], Training Loss: 150.7510\n",
      "Validation Loss: 147.6687, valid error: 148.0856475830078\n",
      "Epoch [14/100], Training Loss: 144.7534\n",
      "Validation Loss: 141.9203, valid error: 142.33380126953125\n",
      "Epoch [15/100], Training Loss: 139.0725\n",
      "Validation Loss: 137.1320, valid error: 137.5384979248047\n",
      "Epoch [16/100], Training Loss: 133.7332\n",
      "Validation Loss: 133.2624, valid error: 133.6700897216797\n",
      "Epoch [17/100], Training Loss: 130.6493\n",
      "Validation Loss: 126.5432, valid error: 126.95110321044922\n",
      "Epoch [18/100], Training Loss: 125.1533\n",
      "Validation Loss: 124.8309, valid error: 125.2444076538086\n",
      "Epoch [19/100], Training Loss: 120.2966\n",
      "Validation Loss: 117.9153, valid error: 118.32402801513672\n",
      "Epoch [20/100], Training Loss: 115.6463\n",
      "Validation Loss: 114.2129, valid error: 114.63042449951172\n",
      "Epoch [21/100], Training Loss: 111.3115\n",
      "Validation Loss: 109.6037, valid error: 110.00868225097656\n",
      "Epoch [22/100], Training Loss: 107.5683\n",
      "Validation Loss: 106.1106, valid error: 106.5204086303711\n",
      "Epoch [23/100], Training Loss: 103.4188\n",
      "Validation Loss: 102.6333, valid error: 103.04267883300781\n",
      "Epoch [24/100], Training Loss: 99.5856\n",
      "Validation Loss: 99.3437, valid error: 99.75289154052734\n",
      "Epoch [25/100], Training Loss: 96.4059\n",
      "Validation Loss: 96.3876, valid error: 96.80361938476562\n",
      "Epoch [26/100], Training Loss: 93.3366\n",
      "Validation Loss: 92.8895, valid error: 93.29715728759766\n",
      "Epoch [27/100], Training Loss: 90.8849\n",
      "Validation Loss: 89.7986, valid error: 90.20497131347656\n",
      "Epoch [28/100], Training Loss: 86.5190\n",
      "Validation Loss: 87.5895, valid error: 87.9925537109375\n",
      "Epoch [29/100], Training Loss: 83.7849\n",
      "Validation Loss: 84.1208, valid error: 84.53111267089844\n",
      "Epoch [30/100], Training Loss: 80.9157\n",
      "Validation Loss: 81.8869, valid error: 82.29850769042969\n",
      "Epoch [31/100], Training Loss: 78.4464\n",
      "Validation Loss: 77.8929, valid error: 78.29918670654297\n",
      "Epoch [32/100], Training Loss: 75.7721\n",
      "Validation Loss: 75.3546, valid error: 75.75914764404297\n",
      "Epoch [33/100], Training Loss: 74.0556\n",
      "Validation Loss: 73.2552, valid error: 73.65808868408203\n",
      "Epoch [34/100], Training Loss: 70.8604\n",
      "Validation Loss: 70.7210, valid error: 71.12955474853516\n",
      "Epoch [35/100], Training Loss: 68.5344\n",
      "Validation Loss: 69.4484, valid error: 69.85586547851562\n",
      "Epoch [36/100], Training Loss: 66.9027\n",
      "Validation Loss: 66.6832, valid error: 67.08879852294922\n",
      "Epoch [37/100], Training Loss: 65.5067\n",
      "Validation Loss: 64.3902, valid error: 64.79322814941406\n",
      "Epoch [38/100], Training Loss: 62.9307\n",
      "Validation Loss: 62.8169, valid error: 63.21998977661133\n",
      "Epoch [39/100], Training Loss: 60.5588\n",
      "Validation Loss: 61.0943, valid error: 61.4981689453125\n",
      "Epoch [40/100], Training Loss: 58.9429\n",
      "Validation Loss: 58.7478, valid error: 59.15102767944336\n",
      "Epoch [41/100], Training Loss: 56.8484\n",
      "Validation Loss: 56.9262, valid error: 57.32954025268555\n",
      "Epoch [42/100], Training Loss: 55.8777\n",
      "Validation Loss: 55.7304, valid error: 56.13102340698242\n",
      "Epoch [43/100], Training Loss: 54.4131\n",
      "Validation Loss: 54.8704, valid error: 55.27301788330078\n",
      "Epoch [44/100], Training Loss: 53.0101\n",
      "Validation Loss: 53.2587, valid error: 53.665016174316406\n",
      "Epoch [45/100], Training Loss: 52.3848\n",
      "Validation Loss: 52.5192, valid error: 52.92304229736328\n",
      "Epoch [46/100], Training Loss: 50.2536\n",
      "Validation Loss: 51.1943, valid error: 51.594398498535156\n",
      "Epoch [47/100], Training Loss: 48.8620\n",
      "Validation Loss: 50.0901, valid error: 50.496681213378906\n",
      "Epoch [48/100], Training Loss: 48.2565\n",
      "Validation Loss: 48.9913, valid error: 49.39776611328125\n",
      "Epoch [49/100], Training Loss: 46.6137\n",
      "Validation Loss: 47.6990, valid error: 48.09696960449219\n",
      "Epoch [50/100], Training Loss: 45.8394\n",
      "Validation Loss: 47.5419, valid error: 47.94137191772461\n",
      "Epoch [51/100], Training Loss: 44.5563\n",
      "Validation Loss: 46.3147, valid error: 46.71663284301758\n",
      "Epoch [52/100], Training Loss: 44.2917\n",
      "Validation Loss: 46.5668, valid error: 46.963802337646484\n",
      "Epoch [53/100], Training Loss: 43.4325\n",
      "Validation Loss: 45.3551, valid error: 45.7591552734375\n",
      "Epoch [54/100], Training Loss: 43.3446\n",
      "Validation Loss: 44.7671, valid error: 45.16703414916992\n",
      "Epoch [55/100], Training Loss: 42.3908\n",
      "Validation Loss: 44.5698, valid error: 44.9680290222168\n",
      "Epoch [56/100], Training Loss: 41.5625\n",
      "Validation Loss: 44.7971, valid error: 45.19765090942383\n",
      "Epoch [57/100], Training Loss: 40.9573\n",
      "Validation Loss: 44.5584, valid error: 44.96360397338867\n",
      "Epoch [58/100], Training Loss: 40.1207\n",
      "Validation Loss: 44.3749, valid error: 44.77288055419922\n",
      "Epoch [59/100], Training Loss: 40.0667\n",
      "Validation Loss: 43.2323, valid error: 43.6328125\n",
      "Epoch [60/100], Training Loss: 39.3818\n",
      "Validation Loss: 42.7496, valid error: 43.14854049682617\n",
      "Epoch [61/100], Training Loss: 38.7729\n",
      "Validation Loss: 44.6218, valid error: 45.019195556640625\n",
      "Epoch [62/100], Training Loss: 38.5102\n",
      "Validation Loss: 42.8671, valid error: 43.271244049072266\n",
      "Epoch [63/100], Training Loss: 37.9117\n",
      "Validation Loss: 42.4319, valid error: 42.834373474121094\n",
      "Epoch [64/100], Training Loss: 37.7785\n",
      "Validation Loss: 41.9585, valid error: 42.35860061645508\n",
      "Epoch [65/100], Training Loss: 37.0941\n",
      "Validation Loss: 42.0879, valid error: 42.48770523071289\n",
      "Epoch [66/100], Training Loss: 36.6807\n",
      "Validation Loss: 41.6804, valid error: 42.078617095947266\n",
      "Epoch [67/100], Training Loss: 36.8127\n",
      "Validation Loss: 41.4414, valid error: 41.84528350830078\n",
      "Epoch [68/100], Training Loss: 36.6156\n",
      "Validation Loss: 41.0821, valid error: 41.47895050048828\n",
      "Epoch [69/100], Training Loss: 35.9033\n",
      "Validation Loss: 41.3898, valid error: 41.786842346191406\n",
      "Epoch [70/100], Training Loss: 35.4506\n",
      "Validation Loss: 40.5146, valid error: 40.91740036010742\n",
      "Epoch [71/100], Training Loss: 35.6415\n",
      "Validation Loss: 40.7161, valid error: 41.1121940612793\n",
      "Epoch [72/100], Training Loss: 35.2789\n",
      "Validation Loss: 40.2898, valid error: 40.68658447265625\n",
      "Epoch [73/100], Training Loss: 34.8158\n",
      "Validation Loss: 40.2008, valid error: 40.60012435913086\n",
      "Epoch [74/100], Training Loss: 34.9273\n",
      "Validation Loss: 39.9869, valid error: 40.382972717285156\n",
      "Epoch [75/100], Training Loss: 34.2733\n",
      "Validation Loss: 40.3096, valid error: 40.70988082885742\n",
      "Epoch [76/100], Training Loss: 34.1458\n",
      "Validation Loss: 40.6864, valid error: 41.086055755615234\n",
      "Epoch [77/100], Training Loss: 34.3004\n",
      "Validation Loss: 39.7322, valid error: 40.13515853881836\n",
      "Epoch [78/100], Training Loss: 33.8563\n",
      "Validation Loss: 39.5756, valid error: 39.97328186035156\n",
      "Epoch [79/100], Training Loss: 33.3474\n",
      "Validation Loss: 40.1753, valid error: 40.57695388793945\n",
      "Epoch [80/100], Training Loss: 33.3404\n",
      "Validation Loss: 39.7457, valid error: 40.14255142211914\n",
      "Epoch [81/100], Training Loss: 33.0368\n",
      "Validation Loss: 39.8180, valid error: 40.22014236450195\n",
      "Epoch [82/100], Training Loss: 32.4559\n",
      "Validation Loss: 39.1804, valid error: 39.58164978027344\n",
      "Epoch [83/100], Training Loss: 32.5690\n",
      "Validation Loss: 39.1738, valid error: 39.5765495300293\n",
      "Epoch [84/100], Training Loss: 32.2647\n",
      "Validation Loss: 38.7969, valid error: 39.206417083740234\n",
      "Epoch [85/100], Training Loss: 32.5862\n",
      "Validation Loss: 39.0397, valid error: 39.441612243652344\n",
      "Epoch [86/100], Training Loss: 32.0969\n",
      "Validation Loss: 38.9991, valid error: 39.39845657348633\n",
      "Epoch [87/100], Training Loss: 31.8072\n",
      "Validation Loss: 38.8083, valid error: 39.20823287963867\n",
      "Epoch [88/100], Training Loss: 31.7203\n",
      "Validation Loss: 39.0605, valid error: 39.458953857421875\n",
      "Epoch [89/100], Training Loss: 31.2223\n",
      "Validation Loss: 39.0153, valid error: 39.41613006591797\n",
      "Epoch [90/100], Training Loss: 31.4967\n",
      "Validation Loss: 39.4985, valid error: 39.90122985839844\n",
      "Epoch [91/100], Training Loss: 31.4751\n",
      "Validation Loss: 39.4314, valid error: 39.831626892089844\n",
      "Epoch [92/100], Training Loss: 30.5419\n",
      "Validation Loss: 38.2759, valid error: 38.676578521728516\n",
      "Epoch [93/100], Training Loss: 31.0106\n",
      "Validation Loss: 38.8636, valid error: 39.26106262207031\n",
      "Epoch [94/100], Training Loss: 31.0108\n",
      "Validation Loss: 38.8715, valid error: 39.27244186401367\n",
      "Epoch [95/100], Training Loss: 30.2198\n",
      "Validation Loss: 38.1994, valid error: 38.59765625\n",
      "Epoch [96/100], Training Loss: 30.4160\n",
      "Validation Loss: 38.8434, valid error: 39.240909576416016\n",
      "Epoch [97/100], Training Loss: 29.7679\n",
      "Validation Loss: 38.6270, valid error: 39.02363204956055\n",
      "Epoch [98/100], Training Loss: 29.7553\n",
      "Validation Loss: 38.1169, valid error: 38.51296615600586\n",
      "Epoch [99/100], Training Loss: 29.3630\n",
      "Validation Loss: 38.0608, valid error: 38.45859146118164\n",
      "Epoch [100/100], Training Loss: 29.2796\n",
      "Validation Loss: 37.6888, valid error: 38.08696365356445\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(256, 1)\n",
    "train_model(model, train_loader, valid_loader, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.6746, valid error: 8.801407814025879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16108.643337847147"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "model = model.to(device='cuda', dtype=torch.float32)\n",
    "validate_model(model, test_loader, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiso131\\AppData\\Local\\Temp\\ipykernel_27416\\2559014824.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: [ 78.87 129.09]\n",
      "label: [218.31 266.07]\n",
      "\n",
      "predict: [52.05 49.55]\n",
      "label: [50.45 49.45]\n",
      "\n",
      "predict: [36.21 49.19 92.84]\n",
      "label: [ 42.1   46.96 129.34]\n",
      "\n",
      "predict: [ 49.64  75.05 157.85 315.27]\n",
      "label: [151.32 200.78 289.72 329.45]\n",
      "\n",
      "predict: [32.78 42.06 37.17 44.52]\n",
      "label: [32.9  41.2  37.81 44.17]\n",
      "\n",
      "predict: [49.29 56.19 73.24 90.45]\n",
      "label: [ 65.54  71.19 107.42 104.77]\n",
      "\n",
      "predict: [24.34 41.61 55.89 38.45 20.98]\n",
      "label: [26.4  43.1  57.32 41.   20.5 ]\n",
      "\n",
      "predict: [ 33.42  41.28  37.92 172.15 236.78]\n",
      "label: [110.76  92.75  43.95 225.54 177.4 ]\n",
      "\n",
      "predict: [  9.69  19.89  61.19 145.39 240.2 ]\n",
      "label: [ 46.38  64.39 135.03 265.85 379.54]\n",
      "\n",
      "predict: [  7.34  38.62  80.54 237.09 427.43]\n",
      "label: [ 63.64 110.74  73.81 205.81 312.6 ]\n",
      "\n",
      "predict: [ 42.98 184.23 208.74 279.35]\n",
      "label: [101.57 153.07 223.97 315.16]\n",
      "\n",
      "predict: [ 47.06 185.22 218.09 317.08]\n",
      "label: [103.16 150.12 210.63 320.74]\n",
      "\n",
      "predict: [ 55.38 191.47 218.77 304.54]\n",
      "label: [109.34 160.81 238.36 334.18]\n",
      "\n",
      "predict: [ 67.81 211.27 245.47 343.69]\n",
      "label: [104.79 156.26 232.97 323.88]\n",
      "\n",
      "predict: [  0.1    1.05   3.23   4.76  33.57  34.43  77.96 125.8   70.71]\n",
      "label: [  0.78   1.17   3.66   6.14  87.08 112.55 194.02 181.58  84.49]\n",
      "\n",
      "predict: [67.35 99.1 ]\n",
      "label: [57.85 78.65]\n",
      "\n",
      "predict: [1239.68  241.26 1157.48  170.35   14.85]\n",
      "label: [994.63 260.29 799.22 145.26  17.58]\n",
      "\n",
      "predict: [20.96 30.99 24.94]\n",
      "label: [22.16 34.29 28.53]\n",
      "\n",
      "predict: [748.91]\n",
      "label: [1962.48]\n",
      "\n",
      "predict: [ 22.02  26.45  37.56  54.67  77.97 204.61 266.32]\n",
      "label: [ 29.5   42.26  60.58  88.25 140.04 202.5  280.18]\n",
      "\n",
      "predict: [ 17.11  19.87  28.14  40.35  44.31  70.8  172.65 224.46]\n",
      "label: [ 21.17  29.44  44.05  68.22 100.66 160.23 231.09 302.48]\n",
      "\n",
      "predict: [ 19.29  30.04  30.84  44.47  59.92  73.82 112.12 202.95 215.56]\n",
      "label: [ 30.16  35.84  39.68  59.63  86.44 123.96 184.96 248.64 288.  ]\n",
      "\n",
      "predict: [314.62]\n",
      "label: [368.3]\n",
      "\n",
      "predict: [1195.3  1182.13 1229.51 1330.93]\n",
      "label: [1481.79 1564.42 1638.29 1667.52]\n",
      "\n",
      "predict: [ 39.63 140.15 172.01 190.01]\n",
      "label: [ 46.71 138.71 205.47 289.82]\n",
      "\n",
      "predict: [224.25 295.5 ]\n",
      "label: [235.67 290.62]\n",
      "\n",
      "predict: [866.8  934.53 957.1 ]\n",
      "label: [ 971.06 1146.88 1227.8 ]\n",
      "\n",
      "predict: [948.92 945.9  912.99]\n",
      "label: [1084.88 1206.07 1302.17]\n",
      "\n",
      "predict: [207.72 519.93 588.16 534.42]\n",
      "label: [141.12 192.02 263.5  339.38]\n",
      "\n",
      "predict: [11.04 22.99 82.87 87.23 76.2  72.66 75.4  85.99]\n",
      "label: [17.23 32.55 72.53 76.72 77.09 55.92 48.15 40.49]\n",
      "\n",
      "predict: [399.45 288.05 117.74 424.69 288.08]\n",
      "label: [102.79  75.35  69.54 166.36 123.13]\n",
      "\n",
      "predict: [1162.48 1226.75 1173.64]\n",
      "label: [523.41 599.21 668.96]\n",
      "\n",
      "predict: [1056.08 1137.72  854.27  387.29  389.06  392.3   258.57  152.28   29.87]\n",
      "label: [1111.26  992.7   839.53  501.61  425.97  395.39  262.51  169.1    13.14]\n",
      "\n",
      "predict: [35.83]\n",
      "label: [33.71]\n",
      "\n",
      "predict: [ 20.8   29.09  38.64  51.64  57.17  64.94 112.14 110.03]\n",
      "label: [ 23.95  31.82  41.85  53.96  60.23  68.92 118.3  119.83]\n",
      "\n",
      "predict: [ 98.22 209.92 250.2  321.58 390.86 410.36 492.44 564.28]\n",
      "label: [134.97 226.01 279.23 377.17 479.93 592.22 719.27 817.19]\n",
      "\n",
      "predict: [1423.7  1635.08]\n",
      "label: [1802.07 1834.06]\n",
      "\n",
      "predict: [561.86 596.22 687.02]\n",
      "label: [627.03 708.5  799.14]\n",
      "\n",
      "predict: [ 597.88  772.35  836.52 1043.12]\n",
      "label: [582.96 666.69 768.53 860.78]\n",
      "\n",
      "predict: [362.37 653.8  689.27 784.14 872.77]\n",
      "label: [415.2  630.66 685.47 835.85 949.01]\n",
      "\n",
      "predict: [112.21  56.17  60.95   3.58   0.     0.     0.  ]\n",
      "label: [99.   51.03 57.4   0.01  0.01  0.    0.  ]\n",
      "\n",
      "predict: [ 68.13  97.12 118.23  79.28  65.79  35.26  27.29  19.64]\n",
      "label: [ 86.87 148.64 165.11 108.26  86.67  38.52  26.93  17.08]\n",
      "\n",
      "predict: [473.66 569.21 620.25 677.46 817.47]\n",
      "label: [264.4  319.78 377.82 438.71 504.79]\n",
      "\n",
      "predict: [458.26 493.37 536.4  467.61]\n",
      "label: [291.08 314.64 377.06 415.79]\n",
      "\n",
      "predict: [ 48.29  27.95  42.84  71.68 124.03  63.09 101.44]\n",
      "label: [42.13 22.77 26.91 27.41 63.37 36.91 68.05]\n",
      "\n",
      "predict: [ 49.77  40.99  46.09  61.31  81.92 100.32 103.34]\n",
      "label: [65.52 38.19 43.84 56.42 75.31 89.48 97.5 ]\n",
      "\n",
      "predict: [153.34]\n",
      "label: [92.99]\n",
      "\n",
      "predict: [1028.43 1023.09]\n",
      "label: [1183.2  1257.11]\n",
      "\n",
      "predict: [ 823.74  341.94  402.4  1336.58 1002.19]\n",
      "label: [1005.36  396.5   557.11 1437.31 1166.26]\n",
      "\n",
      "predict: [1021.35 1023.62 1005.56 1237.14 1166.46]\n",
      "label: [1075.81 1179.15 1333.86 1607.93 1617.11]\n",
      "\n",
      "predict: [ 82.32  92.78 149.21 406.33 415.37]\n",
      "label: [ 93.11 106.89 171.71 411.05 457.38]\n",
      "\n",
      "predict: [1260.3  1225.36 1205.67 1297.47 1363.55]\n",
      "label: [1424.17 1471.08 1523.9  1593.4  1632.5 ]\n",
      "\n",
      "predict: [1395.46 1379.19 1354.1  1365.02 1388.93 1345.45]\n",
      "label: [1671.94 1644.15 1610.9  1573.15 1524.72 1489.71]\n",
      "\n",
      "predict: [33.85 30.38 34.99 29.52]\n",
      "label: [31.11 32.62 37.6  35.4 ]\n",
      "\n",
      "predict: [1263.63 1310.12 1361.22]\n",
      "label: [1191.54 1338.28 1320.74]\n",
      "\n",
      "predict: [1168.61  948.08 1335.02]\n",
      "label: [1023.46  852.33 1263.22]\n",
      "\n",
      "predict: [37.38 58.13 56.6  26.32 28.94 16.35]\n",
      "label: [31.42 46.68 41.8  20.03 25.11 13.36]\n",
      "\n",
      "predict: [ 382.89  233.45  937.67 1389.48 1833.54]\n",
      "label: [ 375.59  221.95  799.94 1069.17 1677.01]\n",
      "\n",
      "predict: [102.01  76.53  65.17  66.63]\n",
      "label: [99.59 73.4  69.9  66.29]\n",
      "\n",
      "predict: [128.72  10.57   1.28]\n",
      "label: [105.76   1.27   1.22]\n",
      "\n",
      "predict: [1384.42 1412.18  815.98 1134.14 1050.04]\n",
      "label: [ 528.71 1014.56  604.17 1100.07  934.58]\n",
      "\n",
      "predict: [1435.65 1615.18 1608.66 1609.85 1655.97]\n",
      "label: [1508.09 1535.68 1543.07 1554.32 1575.56]\n",
      "\n",
      "predict: [ 2.52  6.91 11.5  16.28 36.39 17.9 ]\n",
      "label: [ 2.95  9.55 17.78 22.14 59.16 26.6 ]\n",
      "\n",
      "predict: [ 2.58  3.57  5.76 10.25 15.32 24.59]\n",
      "label: [ 3.66  4.68  8.19 14.54 21.69 33.91]\n",
      "\n",
      "predict: [121.01 112.27  87.03  17.65   0.97   0.     0.92]\n",
      "label: [110.25 142.28 125.25  21.57   2.42   1.78   2.29]\n",
      "\n",
      "predict: [10.75 12.93 15.34 16.22 23.54 31.28 62.8 ]\n",
      "label: [18.37 19.83 26.35 28.37 38.81 49.   94.5 ]\n",
      "\n",
      "predict: [ 524.77  729.79  935.48 1073.65 1186.49 1264.34  599.72]\n",
      "label: [ 502.2   620.95  752.85  879.31  992.42 1084.73  412.94]\n",
      "\n",
      "predict: [85.84 25.79 34.13 54.   70.22 75.16 74.3 ]\n",
      "label: [134.2   29.59  37.77  57.93  75.02  82.46  78.75]\n",
      "\n",
      "predict: [  98.53  454.65  704.12  902.93  951.66  897.64 1081.36 1144.04 1183.77]\n",
      "label: [ 139.03  361.91  537.19  641.6   742.6   800.97  874.62 1034.93 1031.04]\n",
      "\n",
      "predict: [ 4.45  7.85 12.23 23.67 17.77 24.11 41.98 62.9  45.2 ]\n",
      "label: [ 3.9   7.16 11.56 21.23 14.88 21.12 36.5  55.7  37.33]\n",
      "\n",
      "predict: [ 6.83 10.12 11.71 19.9  29.38 35.92 67.02 56.55 66.41]\n",
      "label: [ 6.    8.27  9.45 14.85 21.98 26.98 51.22 43.61 49.43]\n",
      "\n",
      "predict: [ 3.77  6.31  9.05 18.   23.77 27.02 25.47 22.37 42.75]\n",
      "label: [ 3.43  5.18  7.92 14.38 18.49 21.19 19.24 16.31 33.22]\n",
      "\n",
      "predict: [1244.99 1224.97]\n",
      "label: [1052.89 1100.23]\n",
      "\n",
      "predict: [1132.24 1086.44 1176.22 1158.28]\n",
      "label: [ 832.2   906.99 1002.54 1006.3 ]\n",
      "\n",
      "predict: [21.88 32.99 21.47 52.47]\n",
      "label: [21.74 29.34 20.04 45.01]\n",
      "\n",
      "predict: [ 157.66  365.66 1184.83 1480.17  549.69]\n",
      "label: [ 143.18  278.1  1036.2   959.76  346.32]\n",
      "\n",
      "predict: [ 69.11 105.01 189.43 534.08 374.38]\n",
      "label: [ 61.51  86.87 154.95 510.11 342.88]\n",
      "\n",
      "predict: [1078.14  829.57 1083.37  950.87 1028.22]\n",
      "label: [838.44 677.4  830.04 754.32 625.14]\n",
      "\n",
      "predict: [111.29 160.47 135.44 383.38 708.4 ]\n",
      "label: [ 87.15 117.82 118.48 301.14 655.01]\n",
      "\n",
      "predict: [876.26 560.6  447.15 279.46 287.93]\n",
      "label: [596.89 318.4  263.27 248.29 235.71]\n",
      "\n",
      "predict: [878.04 989.9  859.   942.95 803.2 ]\n",
      "label: [750.25 800.28 874.64 922.06 865.95]\n",
      "\n",
      "predict: [867.64 936.09 826.1  877.04 890.92]\n",
      "label: [750.04 817.65 874.84 934.15 991.38]\n",
      "\n",
      "predict: [936.06 996.65 914.95 973.72 962.52]\n",
      "label: [ 757.28  837.7   897.42  964.07 1020.74]\n",
      "\n",
      "predict: [930.45 961.67 884.29 906.61 916.7 ]\n",
      "label: [748.52 823.76 889.73 947.   998.75]\n",
      "\n",
      "predict: [928.01 930.27 835.45 845.43 897.38]\n",
      "label: [ 757.54  837.31  905.01  970.73 1029.23]\n",
      "\n",
      "predict: [33.75 26.04 35.63 26.99 27.23]\n",
      "label: [29.95 28.02 41.57 36.89 34.87]\n",
      "\n",
      "predict: [ 351.35  473.13  185.03  782.98  788.77 1136.33]\n",
      "label: [ 505.61  536.7   237.88  880.25  870.18 1106.22]\n",
      "\n",
      "predict: [15.06 17.56 23.59 32.14 34.44 39.98]\n",
      "label: [14.36 17.67 22.84 29.08 32.35 38.69]\n",
      "\n",
      "predict: [54.24 37.67 24.87 16.91 13.16 12.18  8.79  6.65]\n",
      "label: [47.39 32.35 22.23 15.84 12.12 12.02  9.14  6.66]\n",
      "\n",
      "predict: [19.87 37.55 54.26 55.87 70.74 98.99]\n",
      "label: [19.54 37.5  51.36 55.44 71.99 97.19]\n",
      "\n",
      "predict: [117.74  39.8   13.44   3.01   1.16   0.     0.     0.     0.  ]\n",
      "label: [126.33  33.32   8.26   4.18   1.42   0.35   0.08   0.02   0.  ]\n",
      "\n",
      "predict: [ 70.62 240.26 156.32 114.91 113.65 193.47]\n",
      "label: [ 68.49 255.43 148.91 111.81 117.6  181.47]\n",
      "\n",
      "predict: [306.5  298.54 502.44 711.08 490.71 331.27 676.54]\n",
      "label: [371.63 308.06 555.92 749.09 534.6  359.35 694.27]\n",
      "\n",
      "predict: [10.89  8.98 15.47 24.17 49.53 77.1  59.83]\n",
      "label: [10.69  9.48 15.88 24.34 48.43 75.51 62.87]\n",
      "\n",
      "predict: [ 9.13 10.15 14.5  29.43 39.01 49.73 44.73]\n",
      "label: [ 8.55 11.54 16.78 30.37 40.35 53.74 45.59]\n",
      "\n",
      "predict: [69.01 23.43 45.88 89.58 20.58]\n",
      "label: [65.22 20.11 47.89 87.65 20.87]\n",
      "\n",
      "predict: [ 32.58  43.81  63.26  98.16 176.59 266.02 419.86 529.14]\n",
      "label: [179.02 255.81 338.56 434.2  536.87 630.27 741.54 823.92]\n",
      "\n",
      "predict: [202.85 108.49  87.93 134.2    6.31   0.     0.     0.  ]\n",
      "label: [203.49 110.53  82.77 168.2    0.55   0.2    0.03   0.  ]\n",
      "\n",
      "predict: [29.63 24.76 71.7  29.99 13.87]\n",
      "label: [27.   20.98 63.97 30.5  12.48]\n",
      "\n",
      "predict: [ 98.97 140.5  224.91]\n",
      "label: [ 92.62 134.94 211.25]\n",
      "\n",
      "predict: [695.4  789.02 827.39 885.74]\n",
      "label: [627.32 732.04 814.58 893.62]\n",
      "\n",
      "predict: [144.16 209.23 181.17 181.62]\n",
      "label: [148.76 211.73 191.05 181.48]\n",
      "\n",
      "predict: [ 61.05 170.25 195.92 701.08]\n",
      "label: [ 67.66 165.94 226.33 689.96]\n",
      "\n",
      "predict: [ 412.4  1017.23 1061.74 1182.61]\n",
      "label: [ 392.35 1030.29  930.14  967.6 ]\n",
      "\n",
      "predict: [ 838.39  949.44 1096.95 1172.25 1157.83]\n",
      "label: [553.13 666.41 751.27 842.96 854.26]\n",
      "\n",
      "predict: [152.96 290.16 299.71 459.91 494.44]\n",
      "label: [167.14 279.95 309.31 413.35 445.75]\n",
      "\n",
      "predict: [120.35 119.49  89.64  76.73  62.18]\n",
      "label: [40.14 41.67 32.31 27.14 21.57]\n",
      "\n",
      "predict: [545.56 751.38 632.42 573.26 514.3 ]\n",
      "label: [271.48 365.41 426.72 454.18 438.99]\n",
      "\n",
      "predict: [684.64 811.57 674.79 613.16 556.46]\n",
      "label: [503.36 541.88 611.89 669.37 628.66]\n",
      "\n",
      "predict: [623.98 710.02 732.83 712.05 759.83]\n",
      "label: [562.63 581.19 720.54 771.75 840.6 ]\n",
      "\n",
      "predict: [625.98 648.43 378.04 166.31 569.02]\n",
      "label: [354.26 378.14 191.14 101.31 481.63]\n",
      "\n",
      "predict: [600.36 731.33 669.6  728.4  722.67]\n",
      "label: [416.19 472.43 531.8  591.03 647.8 ]\n",
      "\n",
      "predict: [ 48.87  34.07 141.23 611.46 416.4 ]\n",
      "label: [ 46.23  39.78 172.02 957.92 941.99]\n",
      "\n",
      "predict: [651.37 729.37 675.84 645.12]\n",
      "label: [558.91 570.91 661.1  677.43]\n",
      "\n",
      "predict: [1260.6  1244.9  1203.05 1279.9 ]\n",
      "label: [1213.97 1241.8  1256.56 1258.09]\n",
      "\n",
      "predict: [289.07 245.59 437.48 628.14 376.48 323.91 660.06]\n",
      "label: [338.43 300.31 536.65 730.8  409.46 353.58 838.2 ]\n",
      "\n",
      "predict: [ 5.49  5.52  8.68 14.78 23.51 35.78 31.41]\n",
      "label: [ 8.09 11.13 16.53 28.67 40.35 55.7  48.43]\n",
      "\n",
      "predict: [1077.7  1223.37 1294.62 1465.07 1546.62 1638.76]\n",
      "label: [1250.63 1366.87 1476.62 1580.5  1654.98 1681.97]\n",
      "\n",
      "predict: [173.69 261.72 375.46 492.1  585.02 646.5  727.24]\n",
      "label: [181.33 246.83 322.5  410.67 496.24 593.07 659.41]\n",
      "\n",
      "predict: [ 79.58  80.49 118.51]\n",
      "label: [ 81.87  85.21 117.8 ]\n",
      "\n",
      "predict: [132.53 183.44 169.97 156.  ]\n",
      "label: [149.12 216.55 198.57 193.64]\n",
      "\n",
      "predict: [438.66  42.03   0.47   0.05   0.  ]\n",
      "label: [430.15   0.36   0.17   0.01   0.  ]\n",
      "\n",
      "predict: [243.27 285.2  479.4  546.56 617.39]\n",
      "label: [390.14 448.66 511.84 574.12 631.58]\n",
      "\n",
      "predict: [234.14 284.79 457.55 529.05 618.45]\n",
      "label: [377.54 437.33 496.22 557.26 615.86]\n",
      "\n",
      "predict: [ 797.62  912.95 1041.47]\n",
      "label: [607.85 729.94 867.98]\n",
      "\n",
      "predict: [1405.34 1145.86 1047.73  900.67 1167.09 1406.14 1036.29 1098.93]\n",
      "label: [1780.13 1329.32 1223.98 1022.74 1317.94 1712.07 1203.64 1242.33]\n",
      "\n",
      "predict: [ 8.22 19.84 31.24 20.12 24.01]\n",
      "label: [11.21 28.12 33.56 22.18 26.33]\n",
      "\n",
      "predict: [118.15 121.42  99.34  81.67  62.6 ]\n",
      "label: [138.01 141.21 110.4   92.14  72.79]\n",
      "\n",
      "predict: [ 73.81 104.93 167.09 145.04  67.57]\n",
      "label: [ 78.36 116.41 166.16 151.6   77.03]\n",
      "\n",
      "predict: [38.56 46.31 47.22 47.7  40.61]\n",
      "label: [51.7  55.03 51.9  57.92 48.74]\n",
      "\n",
      "predict: [987.67 397.97 211.28 222.93  28.84  14.29  11.97]\n",
      "label: [1064.29  333.    231.01  233.68   31.32   14.     14.13]\n",
      "\n",
      "predict: [ 529.4   397.94  751.87 1132.68 1327.93]\n",
      "label: [ 376.86  288.28  521.24  884.96 1072.81]\n",
      "\n",
      "predict: [1394.28 1423.77 1442.72 1521.69]\n",
      "label: [1343.56 1349.68 1402.41 1464.56]\n",
      "\n",
      "predict: [1437.36 1513.56 1568.78]\n",
      "label: [1591.8  1445.42 1593.36]\n",
      "\n",
      "predict: [1220.67 1083.74  245.11]\n",
      "label: [1318.44 1367.11  220.79]\n",
      "\n",
      "predict: [1256.13 1191.91 1246.3  1420.69 1432.99]\n",
      "label: [ 980.24 1056.61 1127.64 1285.6  1331.26]\n",
      "\n",
      "predict: [1376.85 1454.24 1446.72 1510.34 1555.23 1570.44 1588.54 1571.06 1572.6 ]\n",
      "label: [1320.79 1397.01 1410.05 1434.76 1461.84 1483.26 1496.11 1490.7  1501.09]\n",
      "\n",
      "predict: [1330.06 1518.47 1567.31]\n",
      "label: [441.9  528.91 553.45]\n",
      "\n",
      "predict: [1364.52 1265.84 1337.42]\n",
      "label: [674.89 536.11 587.91]\n",
      "\n",
      "predict: [1131.14 1216.83 1330.92 1310.84 1293.46]\n",
      "label: [ 901.03 1059.01 1123.21 1183.45 1152.36]\n",
      "\n",
      "predict: [ 88.06 119.68 105.55  94.05 116.87 172.53]\n",
      "label: [ 86.02 105.28  99.09 102.83 117.45 153.4 ]\n",
      "\n",
      "predict: [ 852.79  934.17  972.   1098.32 1161.54]\n",
      "label: [ 835.16  923.61 1001.08 1069.25 1136.08]\n",
      "\n",
      "predict: [ 803.38  933.57  973.24 1081.44 1155.03]\n",
      "label: [ 810.45  903.4   993.17 1074.88 1152.58]\n",
      "\n",
      "predict: [1397.71 1506.03 1497.36 1521.62]\n",
      "label: [1448.53 1469.53 1481.43 1490.3 ]\n",
      "\n",
      "predict: [1446.31 1641.59  924.22 1548.3  1694.64]\n",
      "label: [1441.78 1432.42  916.53 1645.48 1956.18]\n",
      "\n",
      "predict: [ 58.54  67.49  63.08 101.6 ]\n",
      "label: [51.74 57.56 55.17 92.  ]\n",
      "\n",
      "predict: [127.23  57.76  88.83  95.64]\n",
      "label: [118.28  53.16  75.03  85.39]\n",
      "\n",
      "predict: [ 86.27 283.19 309.85 251.54 274.07]\n",
      "label: [168.56 270.89 359.94 428.07 459.07]\n",
      "\n",
      "predict: [ 88.16 250.8  302.91 297.12 332.19]\n",
      "label: [124.31 166.24 299.19 479.89 600.02]\n",
      "\n",
      "predict: [ 63.86 229.09 311.84 361.54 414.5 ]\n",
      "label: [127.73 185.94 265.65 350.2  430.85]\n",
      "\n",
      "predict: [ 78.32 288.49 331.79 327.   382.64]\n",
      "label: [142.6  207.92 282.09 359.79 437.08]\n",
      "\n",
      "predict: [185.5  326.27 306.9  470.1  506.07]\n",
      "label: [192.48 275.82 305.38 412.51 433.83]\n",
      "\n",
      "predict: [390.38 324.98 221.16 208.05 272.59]\n",
      "label: [353.7  320.41 233.48 227.38 297.65]\n",
      "\n",
      "predict: [454.13 516.41 490.61 524.03 606.75]\n",
      "label: [259.09 330.28 405.74 487.52 573.91]\n",
      "\n",
      "predict: [467.96 561.87 492.63 481.32 586.08]\n",
      "label: [267.97 335.23 404.92 479.57 558.84]\n",
      "\n",
      "predict: [471.85 644.85 602.6  535.65 559.41]\n",
      "label: [242.37 307.19 378.99 455.72 540.62]\n",
      "\n",
      "predict: [467.68 679.78 516.02 523.28]\n",
      "label: [301.23 370.24 414.1  545.61]\n",
      "\n",
      "predict: [105.86 152.1  217.27 176.56  96.94]\n",
      "label: [ 89.59 135.01 192.81 161.21  82.29]\n",
      "\n",
      "predict: [ 621.13  775.78 1054.92 1041.81  422.63 1264.55  706.67  241.29]\n",
      "label: [ 549.82  848.08 1334.83 1285.86  499.36 1471.87  850.97  232.19]\n",
      "\n",
      "predict: [ 244.67  710.99 1031.43 1372.33 1148.44]\n",
      "label: [ 265.37  799.99 1033.7  1262.18 1076.52]\n",
      "\n",
      "predict: [ 723.31  998.   1120.2  1050.74 1100.78]\n",
      "label: [ 739.4  1009.77 1154.   1143.43 1128.57]\n",
      "\n",
      "predict: [ 849.47  945.8   969.68 1130.59 1251.92 1293.38 1333.42 1409.07]\n",
      "label: [ 871.58  965.73 1050.05 1125.56 1202.4  1275.52 1352.82 1414.37]\n",
      "\n",
      "predict: [ 856.75  989.29 1041.67 1156.52]\n",
      "label: [ 905.76  998.54 1087.86 1177.84]\n",
      "\n",
      "predict: [1363.49 1477.18 1554.09]\n",
      "label: [1601.65 1644.8  1675.97]\n",
      "\n",
      "predict: [23.19 49.45 41.38 38.71]\n",
      "label: [21.67 45.2  45.67 44.85]\n",
      "\n",
      "predict: [21.83 27.07 27.97 16.46]\n",
      "label: [23.59 31.52 33.25 19.14]\n",
      "\n",
      "predict: [32.67 24.89 18.81 19.59]\n",
      "label: [31.35 25.87 20.01 20.01]\n",
      "\n",
      "predict: [35.44 36.39 40.37]\n",
      "label: [36.3  35.64 39.6 ]\n",
      "\n",
      "predict: [33.58 24.15 23.36 25.94]\n",
      "label: [32.68 25.15 21.03 24.9 ]\n",
      "\n",
      "predict: [90.9  90.53]\n",
      "label: [57.02 60.92]\n",
      "\n",
      "predict: [46.25 47.15 40.37 41.68 55.56]\n",
      "label: [41.57 44.02 40.29 40.86 55.86]\n",
      "\n",
      "predict: [27.93 44.49 33.03 13.92]\n",
      "label: [21.22 32.89 29.09 11.74]\n",
      "\n",
      "predict: [36.52 28.3  29.37 12.55 11.76 21.09]\n",
      "label: [33.73 26.48 28.25 11.98 11.07 21.23]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device='cpu')\n",
    "test_data_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in test_data]\n",
    "\n",
    "for x, y in test_loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(batch_size=x.size(0))\n",
    "        predict, _ = model(x, hidden)\n",
    "\n",
    "        predict_print = (torch.round(predict * 100) / 100).numpy()\n",
    "        #predict_print = label_normalizer.denormalize(predict_print)\n",
    "        y_print = (torch.round(y * 100) / 100).numpy()\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        if max(y_print[0]) > 30:\n",
    "\n",
    "            print(\"predict:\", predict_print[0, :, 0])\n",
    "            print(\"label:\", y_print[0])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
